{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks & Deep Learning\n",
    "在这一节中，我们将构建一个三层的神经网络来处理手写数字识别问题，之后我们将运用AdaGrad、RMSprop、Momentum、Nesterov Momentum和Adam优化算法来加速梯度下降的过程，首先我们先来实现一个简单的神经网络。 \n",
    "\n",
    "## 1. 导入所需的Python库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import load_mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 加载数据并可视化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "先介绍一下在这个实验中所用到的数据库MNIST，MNIST数据集是一个手写体数据集，其中每个手写数字是一张28×28的灰度图片，图片的标记为一个0-9表示的数字。  MNIST数据集一共有60000张图片用来作为训练集，10000张图片来作为测试集。  \n",
    "我们知道一张灰度图片一般是二维的，但是神经网络中的全连接层的输入是一个一维的向量。所以我们需要将一张二维的灰度图片“压扁”成一个一维的向量，具体如下图所示：\n",
    "<img src=\"./images/2d_to_1d.jpg\" width=\"300\" height=\"300\" alt=\"2d_to_1d\" align=center>\n",
    "\n",
    "因此每一个样本都是一个784维的向量。 \n",
    "\n",
    "在处理多分类任务时，我们可以使用softmax来进行处理，这里的手写数字识别任务就是一个多分类任务，共包含有10类，分别用数字0-9表示，而在softmax中，每一类可以表示为一个向量，所以我们需要将类对应的符号标记转化成一个向量表示，这就是one-hot向量，比如，在手写数字识别中，数字0和1对应的one-hot向量分别为： \n",
    "$$one-hot(0)=\\begin{bmatrix}1 \\\\ 0\\\\ 0\\\\ 0\\\\ 0\\\\ 0\\\\ 0\\\\ 0\\\\ 0\\\\ 0\\\\ \\end{bmatrix},one-hot(1)=\\begin{bmatrix}0 \\\\ 1\\\\ 0\\\\ 0\\\\ 0\\\\ 0\\\\ 0\\\\ 0\\\\ 0\\\\ 0\\\\ \\end{bmatrix}$$ \n",
    "在训练集中，我们需要把样本的标记$Y$转化为one-hot向量。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#加载训练集或测试集\n",
    "path = './MNIST Data' #数据集文件所在目录\n",
    "# 加载训练集合测试集\n",
    "# 设置normalization为True，将数据缩放到[0,1]之间\n",
    "# 设置one_hot_label为True，将标签转化为one_hot向量\n",
    "(x_train, y_train), (x_test, y_test) = load_mnist(path, normalize=True, one_hot_label=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of X_train is: (60000, 784)\n",
      "The shape of Y_train is: (60000, 10)\n",
      "The shape of X_test is: (10000, 784)\n",
      "The shape of Y_test is: (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "print('The shape of X_train is:',x_train.shape)\n",
    "print('The shape of Y_train is:',y_train.shape)\n",
    "print('The shape of X_test is:',x_test.shape)\n",
    "print('The shape of Y_test is:',y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面在训练集中找几个图片看一下。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAABrCAYAAABnlHmpAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nGaEKXUQkI1Shi4hkhCp0EZGM+H9xdU7rHm5zXAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one hot 标签： [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.] [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "对应的实际标签： 0 4 1 9\n"
     ]
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "\n",
    "ax1 = fig.add_subplot(141)\n",
    "ax1.imshow(x_train[1,:].reshape(28, 28), cmap='Greys')\n",
    "ax2 = fig.add_subplot(142)\n",
    "ax2.imshow(x_train[2,:].reshape(28,28), cmap='Greys')\n",
    "ax3 = fig.add_subplot(143)\n",
    "ax3.imshow(x_train[3,:].reshape(28,28), cmap='Greys')\n",
    "ax4 = fig.add_subplot(144)\n",
    "ax4.imshow(x_train[4,:].reshape(28,28), cmap='Greys')\n",
    "plt.show()\n",
    "print('one hot 标签：',y_train[1,:],y_train[2,:],y_train[3,:],y_train[4,:])\n",
    "print('对应的实际标签：',np.argmax(y_train[1,:]),np.argmax(y_train[2,:]),np.argmax(y_train[3,:]),np.argmax(y_train[4,:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 全连接神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，我们来搭建一个三层全连接神经网络。网络结构如下图所示：\n",
    "<img src=\"./images/network_architecture.jpg\" width=\"500\" height=\"500\" alt=\"network_architecture\" align=center>\n",
    "\n",
    "对公式所用符号的说明：  \n",
    "一般算神经网络层数不算输入层，所以上图只有3层。用右上角的方括号表示相应的层，所有第1层的权重$W$为$W^{[1]}$，第1层的偏置项$b$为$b^{[1]}$(图中未标出)，第1层的激活值$A$为$A^{[1]}$。  \n",
    "\n",
    "前两层的激活函数使用LeakyRelu，最后一层使用Softmax进行分类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 初始化网络参数\n",
    "\n",
    "我们规定第1层的神经元个数为300，第二层个数为300，最后一层为10.输入向量$X$的维度为784，那么整个网络对应的参数也就可以确定了。  \n",
    "$W^{[1]}$的shape为$(784,300)$，$b^{[1]}$的shape为$(300,)$  \n",
    "$W^{[2]}$的shape为$(300,300)$，$b^{[2]}$的shape为$(300,)$  \n",
    "$W^{[3]}$的shape为$(300,10)$，$b^{[3]}$的shape为$(10,)$  \n",
    "这里使用随机正态分布再乘上比例因子0.01来初始化$W$， 对$b$都初始化为0.  \n",
    "**Hint**: 使用`np.random.randn()`,`np.zeros()`  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_parameters(input_size, hidden_size, output_size, weight_init_std):\n",
    "    \"\"\"\n",
    "    @param input_size:输入向量维度\n",
    "    @param hidden_size:中间神经元个数\n",
    "    @param output_size:输出层神经元个数\n",
    "    @param weight_init_sta:比例因子\n",
    "    \"\"\"\n",
    "    np.random.seed(1)\n",
    "    params = {}\n",
    "    ### START CODE HERE ### \n",
    "    \n",
    "    params['W1'] = None\n",
    "    params['b1'] = None\n",
    "    params['W2'] = None\n",
    "    params['b2'] = None\n",
    "    params['W3'] = None\n",
    "    params['b3'] = None\n",
    "    \n",
    "    ### END CODE HERE ### \n",
    "    \n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 定义神经网络的每一层\n",
    "我们将构成神经网络的层实现为一个单独的类\n",
    "\n",
    "我们先来实现LeakyRelu层"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LeakyRelu层"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "激活函数LeakyRelu的表达式为：\n",
    "$$\n",
    "\\left\\{\\begin{matrix}\n",
    "x & (x>0)\\\\ \n",
    "\\alpha x & (x \\leq 0)\n",
    "\\end{matrix}\\right.\n",
    "$$\n",
    "可以通过上式求出y关于x的导数：\n",
    "$$\n",
    "\\frac{\\partial y}{\\partial x} = \\left\\{\\begin{matrix}\n",
    "1 & (x>0)\\\\ \n",
    "\\alpha & (x \\leq 0)\n",
    "\\end{matrix}\\right.\n",
    "$$\n",
    "\n",
    "如果正向传播时的输入x大于0，则反向传播会将下游的值原封不动地传给上游。反过来，如果正向传播时的x小于等于0，则反向传播中传给上游的信号将乘上一个很小的常数，保证neuron依然起作用。\n",
    "<img src=\"./images/LeakyReLu.png\" width=\"600\" height=\"600\" alt=\"ReLU\" align=center>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LeakyRelu:\n",
    "    def __init__(self):\n",
    "        self.mask = None\n",
    "        self.alpha = 0.1\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.mask = (x <= 0)\n",
    "        out = x.copy()\n",
    "        ### START CODE HERE ### \n",
    "        out[self.mask] = None\n",
    "        ### END CODE HERE ### \n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        ### START CODE HERE ### \n",
    "        dout[self.mask] = None\n",
    "        ### END CODE HERE ### \n",
    "        dx = dout\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试一下效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Relu' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-91c27ba01550>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mrelu\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m2.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3.0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mrelu\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrelu\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Relu' is not defined"
     ]
    }
   ],
   "source": [
    "leakyRelu = LeakyRelu()\n",
    "x = np.array( [[1.0, -0.5], [-2.0, 3.0]] )\n",
    "leakyRelu.forward(x), leakyRelu.backward(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Affine层"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "神经网络的正向传播中，为了计算加权信号的总和，使用了矩阵的乘积运算，回顾一下公式：\n",
    "$$\n",
    "Y = XW + B\n",
    "$$\n",
    "即`Y = np.dot(X, W) + B`  \n",
    "假设X,W,B的形状分别为(2,)、(2,3)、(3,)\n",
    "<img src=\"./images/Affine1.png\">\n",
    "现在将这里进行的求矩阵的乘积与偏置的和的运算用计算图表示出来。将乘积运算用“dot”节点表示的话，则np.dot(X, W) + B的运算如图所示：\n",
    "<img src=\"./images/Affine2.png\">\n",
    "\n",
    "以矩阵为对象的反向传播，按矩阵的各个元素进行计算时，步骤和以标量为对象的计算图相同。实际写\n",
    "一下的话，可以推导得到：\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial X} = \\frac{\\partial L}{\\partial Y} \\cdot W^T\n",
    "$$\n",
    "  \n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W} = X^T \\cdot \\frac{\\partial L}{\\partial Y}\n",
    "$$\n",
    "根据这个式子我们可以写出计算图的反向传播：\n",
    "<img src=\"./images/Affine3.png\">\n",
    "\n",
    "前面介绍的Affine层的输入是以单个数据为对象的，现在我们考虑N个数据一起进行正向传播，计算图如下：\n",
    "<img src=\"./images/Affine4.png\">\n",
    "与刚刚不同的是，现在输入X的形状是(N, 2)。之后就和前面一样，在计算图上进行单纯的矩阵计算。反向传播时，如果注意矩阵的形状，就可以和前面一样推导出$\\frac{\\partial L}{\\partial X}$和$\\frac{\\partial L}{\\partial W}$。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     1,
     20
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Affine:\n",
    "    def __init__(self, W, b):\n",
    "        self.W =W\n",
    "        self.b = b\n",
    "        \n",
    "        self.x = None\n",
    "        self.original_x_shape = None\n",
    "        # 权重和偏置参数的导数\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.original_x_shape = x.shape\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        self.x = x\n",
    "        ### START CODE HERE ### \n",
    "        out = None\n",
    "        ### END CODE HERE ### \n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = np.dot(dout, self.W.T)\n",
    "        ### START CODE HERE ### \n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "        ### END CODE HERE ### \n",
    "        \n",
    "        dx = dx.reshape(*self.original_x_shape)  # 还原输入数据的形状（对应张量）\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Softmax-with-Loss层"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "softmax函数会将输入值正规化后输出，如图所示：\n",
    "<img src=\"./images/softmax.png\">\n",
    "具体公式为\n",
    "$$softmax(x_i)=\\frac{e^{x_i}}{\\sum_{j=1}^{C}{e^{x_j}}}$$ \n",
    "$x_i$表示为向量$x$的第$i$个分量。  $softmax$函数这里直接给出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    x = x.T\n",
    "    x = x - np.max(x, axis=0)\n",
    "    y = np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "    return y.T "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在softmax中，我们一般使用的损失函数为交叉熵函数：  \n",
    "$$L(\\hat{y},y)=-\\sum_{j=1}^{C}{y_jlog\\hat{y_j}}$$  \n",
    "其中，$C$是类别的数量，在本次实验中即为10.  \n",
    "成本函数为：  \n",
    "$$J(W^{[1]},b^{[1]},...)=\\frac{1}{m}\\sum_{i=1}^{m}{L(\\hat{y}^{(i)},y^{(i)})}$$  \n",
    "这里$m$是mini-batch的大小。因为训练集有60000个，我们不能直接把$(60000,784)$大小的输入直接放入神经网络计算。因此，每次就选择一部分来进行前向传播，所以输入的大小是$(m,784)$.\n",
    "**Hint**: 最后的Loss是个标量 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_entropy_error(pred, y):\n",
    "    if pred.ndim == 1:\n",
    "        y = y.reshape(1, y.size)\n",
    "        pred = pred.reshape(1, pred.size)\n",
    "        \n",
    "    # 监督数据是one-hot-vector的情况下，转换为正确解标签的索引\n",
    "    if y.size == pred.size:\n",
    "        y = y.argmax(axis=1)\n",
    "             \n",
    "    batch_size = pred.shape[0]\n",
    "    \n",
    "    res = None\n",
    "\n",
    "    res = -np.sum(np.log(pred[:, y] + 1e-7)) / batch_size\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面来实现Softmax 层。考虑到这里也包含作为损失函数的交叉熵误差（cross entropy error），所以称为“Softmax-with-Loss 层”。Softmax-with-Loss 层（Softmax函数和交叉熵误差）的计算图如图所示。\n",
    "<img src=\"./images/softmax-cross.png\">\n",
    "\n",
    "可以看到，Softmax-with-Loss 层有些复杂。这里只给出了最终结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.loss = None\n",
    "        self.pred = None # softmax的输出\n",
    "        self.y = None # 监督数据\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        self.y = y\n",
    "        self.pred = softmax(x)\n",
    "        self.loss = cross_entropy_error(self.pred, self.y)\n",
    "        \n",
    "        return self.loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        batch_size = self.y.shape[0]\n",
    "        if self.y.size == self.pred.size: # 监督数据是one-hot-vector的情况\n",
    "            dx = (self.pred - self.y) / batch_size\n",
    "        else:\n",
    "            dx = self.pred.copy()\n",
    "            dx[np.arange(batch_size), self.y] -= 1\n",
    "            dx = dx / batch_size\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 搭积木\n",
    "现在我们把之前的每一层组合在一起就能搭成我们自己的三层神经网络了。\n",
    "神经网络学习的步骤大致有5步：\n",
    "1. 初始化权重\n",
    "2. 随机选择一部分数据\n",
    "3. 计算梯度\n",
    "4. 更新参数\n",
    "5. 重复步骤2,3,4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     1,
     18,
     27,
     35,
     48
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TwoLayerNet:\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std = 0.01):\n",
    "        \n",
    "        # 初始化权重\n",
    "        self.params = initialize_parameters(input_size, hidden_size, output_size, weight_init_std)\n",
    "        # 记录训练次数 adam里要用\n",
    "        self.t = 0\n",
    "\n",
    "        # 生成层\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1'])\n",
    "        self.layers['LeakyRelu1'] = LeakyRelu()\n",
    "        self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2'])\n",
    "        self.layers['LeakyRelu2'] = LeakyRelu()\n",
    "        self.layers['Affine3'] = Affine(self.params['W3'], self.params['b3'])\n",
    "    \n",
    "        self.lastLayer = SoftmaxWithLoss()\n",
    "        \n",
    "    def predict(self, x):\n",
    "        # 前向传播\n",
    "        pred = x.copy()\n",
    "        for layer in self.layers.values():\n",
    "            ### START CODE HERE ### \n",
    "            pred = None\n",
    "            ### END CODE HERE ### \n",
    "        return pred\n",
    "        \n",
    "    def loss(self, x, y):\n",
    "        # 计算交叉熵误差\n",
    "        ### START CODE HERE ### \n",
    "        pred = None\n",
    "        loss = None\n",
    "        ### END CODE HERE ### \n",
    "        return loss\n",
    "    \n",
    "    def accuracy(self, x, y):\n",
    "        # 输入数据x和标签y，输出当前神经网络的预测准确率\n",
    "        accuracy = None\n",
    "        pred = self.predict(x)\n",
    "        pred = np.argmax(pred, axis=1)\n",
    "        if y.ndim != 1:\n",
    "            y = np.argmax(y, axis=1)\n",
    "        \n",
    "        accuracy = np.sum(pred == y) / float(x.shape[0])\n",
    "\n",
    "        return accuracy\n",
    "        \n",
    "    def gradient(self, x, y):\n",
    "        # 前向传播\n",
    "        self.loss(x, y)\n",
    "\n",
    "        # 反向传播\n",
    "        dout = 1\n",
    "        dout = self.lastLayer.backward(dout)\n",
    "        \n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        # 设定\n",
    "        grads = {}\n",
    "        grads['W1'], grads['b1'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\n",
    "        grads['W2'], grads['b2'] = self.layers['Affine2'].dW, self.layers['Affine2'].db\n",
    "        grads['W3'], grads['b3'] = self.layers['Affine3'].dW, self.layers['Affine3'].db\n",
    "        \n",
    "        return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 进行训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先定义参数更新函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_parameters(network, grads, learning_rate=0.001):\n",
    "    \"\"\"\n",
    "    使用梯度下降法更新network的参数\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ### \n",
    "    \n",
    "    for key in ('W1', 'b1', 'W2', 'b2', 'W3', 'b3'):\n",
    "        network.params[key] -= None\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后定义训练函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_network(network, update_params_method, iters_num, train_size, batch_size, learning_rate):\n",
    "    train_loss_list = []\n",
    "    train_acc_list = []\n",
    "    test_acc_list = []\n",
    "\n",
    "    iter_per_epoch = max(train_size / batch_size, 1)\n",
    "\n",
    "    for i in range(iters_num):\n",
    "        batch_mask = np.random.choice(train_size, batch_size)\n",
    "        x_batch = x_train[batch_mask]\n",
    "        t_batch = y_train[batch_mask]\n",
    "        network.t += 1\n",
    "\n",
    "        # 计算梯度\n",
    "        grad = network.gradient(x_batch, t_batch)\n",
    "\n",
    "        # 更新梯度\n",
    "        update_params_method(network, grad, learning_rate)\n",
    "\n",
    "        loss = network.loss(x_batch, t_batch)\n",
    "        train_loss_list.append(loss)\n",
    "\n",
    "        if i % iter_per_epoch == 0:\n",
    "            train_acc = network.accuracy(x_train, y_train)\n",
    "            test_acc = network.accuracy(x_test, y_test)\n",
    "            train_acc_list.append(train_acc)\n",
    "            test_acc_list.append(test_acc)\n",
    "            print(\"Train acc:{:<.6f}\\tTest acc:{:<.6f}\".format(train_acc, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 读入数据\n",
    "(x_train, y_train), (x_test, y_test) = load_mnist(path, normalize=True, one_hot_label=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "network = TwoLayerNet(input_size=784, hidden_size=300, output_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "iters_num = 10000\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.1\n",
    "train_network(network, update_parameters, iters_num, train_size, batch_size, learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 梯度下降优化算法\n",
    "下面回顾几种上课讲过的优化算法，注意它们之间的差异与联系。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5.1AdaGrad  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当$J(\\theta)$收敛到最低值附近时，因为步长$\\alpha$大小固定的原因，$J(\\theta)$会在最低值附近徘徊，而不能到达最低值。因此，AdaGrad的想法是随着迭代次数的增加降低学习率$\\alpha$，学习率$\\alpha$衰减的方式是\n",
    "$$\n",
    "\\alpha^t = \\frac{\\alpha}{\\sqrt{t+1}}\n",
    "$$\n",
    "其中t表示第t次迭代。\n",
    "\n",
    "如果梯度数值小，$J(\\theta)$的移动步长小，$J(\\theta)$在坡度平缓的区域内下降速度会变慢。AdaGrad使用均方根来加快$J(\\theta)$在平缓区域的下降速度。均方根的表示为\n",
    "$$\n",
    "\\sigma^t = \\sqrt{\\frac{1}{t+1} \\sum_{i=0}^{t}(g^i)2}\n",
    "$$\n",
    "其中$g^i$表示历史的梯度值。AdaGrad 的更新参数公式是\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\theta^{t+1} & := \\theta^t - \\frac{\\alpha^t}{\\sigma^t} g^t \\\\\n",
    "& := \\theta^t - \\frac{\\alpha}{\\sqrt{\\sum_{i=0}^t (g^i)^2}} g^t\n",
    "\\end{aligned}\n",
    "$$\n",
    "在坡度平缓的区域，均方根的数值小，梯度除以一个数值小的数会变大，从而加大了$J(\\theta)$移动步长，也因此加快梯度下降速度。但是，AdaGrad的缺点是，随着迭代次数的增大，均方根会越来越大，梯度趋近于0，导致训练提前停止。为了防止分母为0，我们给分母加上一个小数值$\\epsilon =10^{-7}$。\n",
    "$$\n",
    "\\theta^{t+1} := \\theta^t - \\frac{\\alpha}{\\sqrt{\\sum_{i=0}^t (g^i)^2} + \\epsilon} g^t\n",
    "$$\n",
    "\n",
    "我们可以看到分母里会计算所有历史梯度值的平方和，所以在实现的时候不用保存所有的历史梯度值，只需要保存一个纪录所有历史梯度平方和的值即可。每个参数的历史梯度和初始值为0。   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_grads_squared(network):\n",
    "    \"\"\"\n",
    "    初始化历史梯度和\n",
    "    \"\"\"\n",
    "    grads_squared = {}\n",
    "    \n",
    "    for key in ('W1', 'b1', 'W2', 'b2', 'W3', 'b3'):\n",
    "        \n",
    "        grads_squared[key] = np.zeros(network.params[key].shape)\n",
    "        \n",
    "    return grads_squared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "network = TwoLayerNet(input_size=784, hidden_size=300, output_size=10)\n",
    "grads_squared = initialize_grads_squared(network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_parameters_with_adagrad(network, grads, learning_rate=0.001, epsilon = 1e-7):\n",
    "    for key in ('W1', 'b1', 'W2', 'b2', 'W3', 'b3'):\n",
    "        ### START CODE HERE ### \n",
    "        \n",
    "        #计算历史梯度平方和\n",
    "        grads_squared[key] += None   \n",
    "        network.params[key] -= None\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "iters_num = 10000\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.001\n",
    "train_network(network, update_parameters_with_adagrad, iters_num, train_size, batch_size, learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5.2 RMSprop  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从AdaGrad算法的公式可看出，所有时刻的梯度都对当前的参数更新有影响。如果早先的梯度并不是一个正确的方向，那么这些糟糕的梯度还是会影响到当前的参数更新。因此，RMSprop相当于就是只记录当前时刻前的某一段历史梯度和而不是所有历史梯度和。  \n",
    "RMSprop算法的公式如下：  \n",
    "$$\n",
    " u^0 = 0 \\\\\n",
    " u^{t+1} = \\rho u^t + (1-\\rho) [\\nabla J(\\theta ^t)]^2 \\\\ \n",
    " \\theta^{t+1} = \\theta^t - \\frac{\\alpha}{\\sqrt{u^{t+1}}+\\epsilon}\\nabla J(\\theta ^t) \n",
    "$$\n",
    "这里$\\rho$是超参数，一般设为0.999，也不会调它。$\\epsilon$是防止分母为0。另外值得注意的是，因为要整合这几个算法在一起，而Adam算法又融合了各种算法，所以，关于优化算法的超参数的命名与Adam里保持一致，公式里的$\\rho$用下面参数`beta`代替。这些算法几乎都要保存一些变量，它们的初始化基本与AdaGrad初始化的方法一致，所以这部分初始化的代码就不重复了。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_parameters_with_rmsprop(network, grads, learning_rate=0.001, epsilon = 1e-7, beta=0.999):\n",
    "    for key in ('W1', 'b1', 'W2', 'b2', 'W3', 'b3'):\n",
    "        ### START CODE HERE ### \n",
    "        \n",
    "        #公式里的u就是这里的 grads_squared         \n",
    "        grads_squared[key] = None\n",
    "        network.params[key] -=  None\n",
    "        \n",
    "        ### END CODE HERE ###\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "network = TwoLayerNet(input_size=784, hidden_size=300, output_size=10)\n",
    "grads_squared = initialize_grads_squared(network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "iters_num = 10000\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.001\n",
    "train_network(network, update_parameters_with_rmsprop, iters_num, train_size, batch_size, learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5.3 Momentum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "动量梯度下降（Gradient Descent with Momentum）基本思想就是计算梯度的指数加权平均数，并利用该指数加权平均数更新权重。具体过程为：\n",
    "$$\n",
    "v^0 = 0 \\\\\n",
    "v^{t+1}  = \\rho v^t +\\alpha \\nabla J(\\theta ^t) \\\\\n",
    "\\theta^{t+1} = \\theta ^t - v^{t+1}\n",
    "$$\n",
    "\n",
    "这里的$\\rho$一般取0.9。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_velocity(network):\n",
    "    v = {}\n",
    "    for key in ('W1', 'b1', 'W2', 'b2', 'W3', 'b3'):\n",
    "        v[key] = np.zeros((network.params[key]).shape) \n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "network = TwoLayerNet(input_size=784, hidden_size=300, output_size=10)\n",
    "v = initialize_velocity(network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_parameters_with_momentum(network, grads, learning_rate=0.001, beta=0.9):\n",
    "    for key in ('W1', 'b1', 'W2', 'b2', 'W3', 'b3'):\n",
    "        ### START CODE HERE ### \n",
    "        \n",
    "        #公式里的u就是这里的 grads_squared         \n",
    "        v[key] = None\n",
    "        network.params[key] -= None\n",
    "        \n",
    "        ### END CODE HERE ###\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "iters_num = 10000\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.001\n",
    "train_network(network, update_parameters_with_momentum, iters_num, train_size, batch_size, learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5.4 Nesterov Momentum  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nesterov Momentum算法与Momentum不同的是在于，它会提前计算一个在速度作用后的梯度。具体算法如下：\n",
    "$$\n",
    "v^{t+1} = \\rho v^t + \\alpha \\nabla J(\\theta ^t - \\rho v^t) \\\\\n",
    "\\theta^{t+1} = \\theta ^t - v^{t+1}\n",
    "$$\n",
    "但是在实现的时候，我们是不会算一次$J(\\theta ^t)$再算一次$\\nabla J(\\theta ^t - \\rho v^t)$的。具体编程实现时上式等价于下式：\n",
    "$$\n",
    " v^{t+1} = \\rho v^t + \\alpha \\nabla J(\\theta ^t) \\\\\n",
    " \\theta^{t+1} = \\theta ^t - \\rho v^{t+1} - \\alpha \\nabla J(\\theta ^t)\n",
    "$$\n",
    "这里的$\\rho$一般取0.9。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_parameters_with_nesterov_momentum(network, grads, learning_rate=0.001, beta=0.9):\n",
    "    for key in ('W1', 'b1', 'W2', 'b2', 'W3', 'b3'):\n",
    "        ### START CODE HERE ### \n",
    "                \n",
    "        v[key] = None\n",
    "        network.params[key] -= None\n",
    "        \n",
    "        ### END CODE HERE ###\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "network = TwoLayerNet(input_size=784, hidden_size=300, output_size=10)\n",
    "v = initialize_velocity(network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "iters_num = 10000\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.001\n",
    "train_network(network, update_parameters_with_nesterov_momentum, iters_num, train_size, batch_size, learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5.5 Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adam优化算法（Adaptive Moment Estimation）是将Momentum和RMSprop结合在一起的算法，具体过程如下\n",
    "$$\n",
    "u^0 = 0 \\\\\n",
    "v^0 = 0 \\\\\n",
    "u^{t+1}  = \\rho_2 u^t +(1-\\rho_2) [\\nabla J(\\theta ^t)]^2 \\\\\n",
    "v^{t+1}  = \\rho_1 v^t + (1-\\rho_1)\\nabla J(\\theta ^t) \\\\\n",
    "\\theta^{t+1} = \\theta ^t - \\frac{\\alpha}{\\sqrt{u^{t+1}}+\\epsilon}v^{t+1}\n",
    "$$\n",
    "从上式可以看到，在最开始更新时，$u^{t},v^{t}$都是很小的。所以需要对早期的更新进行一个bias correction。完整公式如下\n",
    "$$\n",
    "u^0 = 0 \\\\\n",
    "v^0 = 0 \\\\\n",
    "u^{t+1}  = \\rho_2 u^t +(1-\\rho_2) [\\nabla J(\\theta ^t)]^2 \\\\\n",
    "u^{t+1}_{corrected} = \\frac{u^{t+1}}{1-\\rho_2^t} \\\\\n",
    "v^{t+1}  = \\rho_1 v^t + (1-\\rho_1)\\nabla J(\\theta ^t) \\\\\n",
    "v^{t+1}_{corrected} = \\frac{v^{t+1}}{1-\\rho_1^t} \\\\\n",
    "\\theta^{t+1} = \\theta ^t - \\frac{\\alpha}{\\sqrt{u^{t+1}_{corrected}}+\\epsilon}v^{t+1}_{corrected}\n",
    "$$\n",
    "\n",
    "其中，一般设$\\rho_1=0.9,\\rho_2=0.999$.$\\epsilon$也是防止分母过小或等于0.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_adam(network) :\n",
    "    v = {}\n",
    "    u = {}\n",
    "\n",
    "    for key in ('W1', 'b1', 'W2', 'b2', 'W3', 'b3'):\n",
    "        v[key] = np.zeros(np.shape(network.params[key]))\n",
    "        u[key] = np.zeros(np.shape(network.params[key]))\n",
    "            \n",
    "    return v, u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "network = TwoLayerNet(input_size=784, hidden_size=300, output_size=10)\n",
    "v, u = initialize_adam(network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_parameters_with_adam(network, grads, learning_rate=0.001, epsilon=1e-7, beta1=0.9, beta2=0.999):\n",
    "    v_corrected = {}\n",
    "    u_corrected = {} \n",
    "    t = network.t #当前迭代次数\n",
    "    for key in ('W1', 'b1', 'W2', 'b2', 'W3', 'b3'):\n",
    "        ### START CODE HERE ### \n",
    "                \n",
    "        v[key] = None\n",
    "        v_corrected[key] = None\n",
    "        \n",
    "        u[key] = None\n",
    "        u_corrected[key] = None\n",
    "        \n",
    "        network.params[key] -= None\n",
    "        \n",
    "        ### END CODE HERE ###\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "iters_num = 10000\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.001\n",
    "train_network(network, update_parameters_with_adam, iters_num, train_size, batch_size, learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5.6 AdaBelief"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adam优化算法中，梯度更新的方向是 $\\frac{v_{t+1}}{\\sqrt{u_{t+1}}}$, 其中 $u_{t+1}$ 是 $[\\nabla J(\\theta ^t)]^2$ 的指数移动平均(exponential moving average,EMA)。\n",
    "\n",
    "AdaBelief优化算法将梯度方向改为了 $\\frac{v_{t+1}}{\\sqrt{s_{t+1}}}$, 其中 $s_{t+1}$ 是 $[\\nabla J(\\theta ^t)-v_{t}]^2$ 的指数移动平均，AdaBelief的算法过程具体如下：\n",
    "$$\n",
    "s^0 = 0 \\\\\n",
    "v^0 = 0 \\\\\n",
    "v^{t+1}  = \\rho_1 v^t + (1-\\rho_1)\\nabla J(\\theta ^t) \\\\\n",
    "s^{t+1}  = \\rho_2 s^t +(1-\\rho_2) [\\nabla J(\\theta ^t)-v_{t+1}]^2 \\\\\n",
    "\\theta^{t+1} = \\theta ^t - \\frac{\\alpha}{\\sqrt{s^{t+1}}+\\epsilon}v^{t+1}\n",
    "$$\n",
    "\n",
    "与Adam优化算法同理，从上式可以看到，AdaBelief优化算法在最开始更新时，$s^{t},v^{t}$都是很小的。所以需要对早期的更新进行一个bias correction。完整公式如下\n",
    "$$\n",
    "s^0 = 0 \\\\\n",
    "v^0 = 0 \\\\\n",
    "v^{t+1}  = \\rho_1 v^t + (1-\\rho_1)\\nabla J(\\theta ^t) \\\\\n",
    "v^{t+1}_{corrected} = \\frac{v^{t+1}}{1-\\rho_1^t} \\\\\n",
    "s^{t+1}  = \\rho_2 s^t +(1-\\rho_2) [\\nabla J(\\theta ^t)-v_{t+1}]^2 \\\\\n",
    "s^{t+1}_{corrected} = \\frac{s^{t+1}}{1-\\rho_2^t} \\\\\n",
    "\\theta^{t+1} = \\theta ^t - \\frac{\\alpha}{\\sqrt{s^{t+1}_{corrected}}+\\epsilon}v^{t+1}_{corrected}\n",
    "$$\n",
    "\n",
    "其中，一般设$\\rho_1=0.9,\\rho_2=0.999$.$\\epsilon$也是防止分母过小或等于0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_adambelief(network) :\n",
    "    v = {}\n",
    "    s = {}\n",
    "\n",
    "    for key in ('W1', 'b1', 'W2', 'b2', 'W3', 'b3'):\n",
    "        v[key] = np.zeros(np.shape(network.params[key]))\n",
    "        s[key] = np.zeros(np.shape(network.params[key]))\n",
    "            \n",
    "    return v, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "network = TwoLayerNet(input_size=784, hidden_size=300, output_size=10)\n",
    "v, s = initialize_adambelief(network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_parameters_with_adambelief(network, grads, learning_rate=0.001, epsilon=1e-7, beta1=0.9, beta2=0.999):\n",
    "    v_corrected = {}\n",
    "    s_corrected = {} \n",
    "    t = network.t #当前迭代次数\n",
    "    for key in ('W1', 'b1', 'W2', 'b2', 'W3', 'b3'):\n",
    "        ### START CODE HERE ### \n",
    "                \n",
    "        v[key] = None\n",
    "        v_corrected[key] = None\n",
    "        \n",
    "        s[key] = None\n",
    "        s_corrected[key] = None\n",
    "        \n",
    "        network.params[key] -= None\n",
    "        \n",
    "        ### END CODE HERE ###\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "iters_num = 10000\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.001\n",
    "train_network(network, update_parameters_with_adambelief, iters_num, train_size, batch_size, learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 总结\n",
    "本次实验完整搭建了一个三层的全连接网络，使用了各种梯度更新优化算法训练MNIST数据集。  \n",
    "可以试试再把准确度提高一点？"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
