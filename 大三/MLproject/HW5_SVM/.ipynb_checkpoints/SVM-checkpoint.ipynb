{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZpVfPosXpDwd"
   },
   "source": [
    "**一 SVM简单介绍** \n",
    "\n",
    "支持向量机(Support Vector Machine，SVM)是Corinna Cortes和Vapnik等于1995年首先提出的，它在解决小样本、非线性及高维模式识别中表现出许多特有的优势，并能够推广应用到函数拟合等其他机器学习问题中。 在机器学习中，支持向量机是与相关的学习算法有关的监督学习模型，可以分析数据、识别模式，用于分类和回归分析。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bW4VBEbBpDwg"
   },
   "source": [
    "**二 函数间隔与几何间隔**  \n",
    "\n",
    "对一个数据点进行分类，当超平面离数据点的“间隔”越大，分类的确信度（confidence）也越大。所以，为了使得分类的确信度尽量高，需要让所选择的超平面能够最大化这个“间隔”值。这个间隔就是下图中的Gap的一半。\n",
    "![2.png](image/2.png)\n",
    "\n",
    "\n",
    "\n",
    "**2.1 函数间隔**<br>\n",
    "定义函数间隔为：\n",
    "$$\\hat\\gamma=y(w^Tx+b)=yf(x)$$\n",
    "而超平面(w，b)关于T中所有样本点($x_i$，$y_i$)的函数间隔最小值（其中，x是特征，y是结果标签，i表示第i个样本），便为超平面(w, b)关于训练数据集T的函数间隔。 但这样定义的函数间隔有问题，即如果成比例的改变w和b（如将它们改成2w和2b），则函数间隔的值f(x)却变成了原来的2倍（虽然此时超平面没有改变），所以只有函数间隔还不够，我们要引入几何间隔。\n",
    "![1.png](image/1.png) \n",
    "\n",
    "**2.2 几何间隔**<br>\n",
    "在几何学里，点($x_i$，$y_i$)到直线$ax+by+c=0$的距离公式:\n",
    "$$d(x_i, y_i) = \\frac{|ax_i+by_i+c|}{\\sqrt{a^{2}+b^{2}}}$$\n",
    "所以在二维空间中，几何间隔就是点到直线的距离；在三维空间中，几何间隔是点到空间上一平面的距离；而在SVM中，平面变成了超平面，几何间隔便是样本点到超平面的距离：\n",
    "$$\\gamma_{i}=y_{i}(\\frac{w^T}{\\Vert w\\Vert}x_{i}+\\frac{b}{\\Vert w\\Vert})$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_XtDLxHBpDwh"
   },
   "source": [
    "**三 最大间隔分类器**  \n",
    "\n",
    "\n",
    "如下图所示，中间的实线便是寻找到的最优超平面（Optimal Hyper Plane），其到两条虚线边界的距离相等，虚线边界上的点到最优超平面的距离便是几何间隔，两条虚线间隔边界之间的距离等于2倍的几何间隔，而虚线间隔边界上的点则是支持向量。而对于所有不是支持向量的点，有如下关系：\n",
    "$y(w^Tx+b)>1$\n",
    "![3.png](image/3.png)\n",
    "\n",
    "函数间隔不适合用来最大化间隔值，因为在超平面固定以后，可以等比例地缩放w的长度和b的值，这样可以使得的值任意大，亦即函数间隔可以在超平面保持不变的情况下被取得任意大。但几何间隔因为除上了${\\Vert w\\Vert}$，使得在缩放w和b的时候几何间隔的值是不会改变的，它只随着超平面的变动而变动，因此，这是更加合适的一个间隔。换言之，这里要找的最大间隔分类超平面中的“间隔”指的是几何间隔。 于是最大间隔分类器（maximum margin classifier）的目标函数可以定义为： 同时需满足一些条件，根据间隔的定义，有\n",
    "$$y_i(w^Tx_i+b)=\\hat\\gamma_i>=\\hat\\gamma,i=1,2,...,n$$\n",
    "距离超平面最近的这几个训练样本点使得等式成立，它们被称为**支持向量**，这些支持向量到到超平面的距离是$\\frac{1}{w}$\n",
    "从而上述目标函数转化成了：\n",
    "$$max\\frac{1}{\\Vert w\\Vert},s.t.\\ y_i(w^Tx_i+b)>=1,i=1,2,...,n$$\n",
    "\n",
    "相当于在相应的约束条件下，最大化这个$\\frac{1}{\\Vert w\\Vert}$值，而$\\frac{1}{\\Vert w\\Vert}$便是要求解的几何间隔。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "itajtXgSpDwj"
   },
   "source": [
    "**通过以上的介绍，我们得出支持向量机的目标函数，但往往这个问题不是那么容易解决，所以需要将其转化为其对偶形式求解，往往对对偶形式求解会比直接对原问题求解方便很多。**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q1qERHrRpDwk"
   },
   "source": [
    "**四 原始问题到对偶问题的求解**   \n",
    "\n",
    "接着考虑之前得到的目标函数：\n",
    "$$max\\frac{1}{\\Vert w\\Vert}$$ $$s.t.\\ y_i(w^Tx_i+b)>=1,i=1,2,...,n$$\n",
    "由于求$\\frac{1}{\\Vert w\\Vert}$的最大值相当于求${\\Vert w\\Vert}$的最小值，也相当于求$\\frac{1}{2}{\\Vert w\\Vert}^2$的最小值，所以上述目标函数等价于：\n",
    "$$min\\frac{1}{2}{\\Vert w\\Vert}^2$$ $$s.t.\\ y_i(w^Tx_i+b)>=1,i=1,2,...,n$$\n",
    "\n",
    "此外，由于这个问题的特殊结构，还可以通过拉格朗日对偶性（Lagrange Duality）变换到对偶变量 (dual variable) 的优化问题，即通过求解与原问题等价的对偶问题（dual problem）得到原始问题的最优解，这就是线性可分条件下支持向量机的对偶算法，这样做的优点在于：一者对偶问题往往更容易求解；二者可以自然的引入核函数，进而推广到非线性分类问题。 那什么是拉格朗日对偶性呢？简单来讲，通过给每一个约束条件加上一个**拉格朗日乘子**（Lagrange multiplier），定义拉格朗日函数（通过拉格朗日函数将约束条件融合到目标函数里去，从而只用一个函数表达式便能清楚的表达出我们的问题）：\n",
    "$$L\\left(w,b,\\alpha\\right)=\\frac{1}{2}{\\Vert w\\Vert}^2-\\sum_{i=1}^{n}\\alpha_i{\\lgroup}y_i\\left(w^Tx_i+b\\right)-1\\rgroup$$\n",
    "然后令：\n",
    "$$\\theta\\left(w\\right)=max\\ L\\left(w,b,\\alpha\\right),\\alpha_i>=0$$  \n",
    "\n",
    "容易验证，当某个约束条件不满足时，例如$y_i\\left(w^Tx_i+b\\right)<1$，那么显然有$\\theta\\left(w\\right)=\\infty$（只要令$\\alpha_i=\\infty$即可）。而当所有约束条件都满足时，则最优值为$\\theta\\left(w\\right)=\\frac{1}{2}{w}^2$，亦即最初要最小化的量。 因此，在要求约束条件得到满足的情况下最小化$\\frac{1}{2}{w}^2$，实际上等价于直接最小化$\\theta\\left(w\\right)$（当然，这里也有约束条件，就是$\\alpha_i>=0,i=1,...,n)$，因为如果约束条件没有得到满足，会等于无穷大，自然不会是我们所要求的最小值。 具体写出来，目标函数变成了：\n",
    "$$\\min\\limits_{w,b}\\theta\\left(w\\right)=\\min\\limits_{w,b}                              \\max\\limits_{\\alpha_i>=0}L\\left(w,b,\\alpha\\right)=p^*$$\n",
    "这里用$p^*$表示这个问题的最优值，且和最初的问题是等价的。如果直接求解，那么一上来便得面对w和b两个参数，而又是不等式约束，这个求解过程不好做。不妨把最小和最大的位置交换一下，变成：\n",
    "$$\\max\\limits_{\\alpha_i>=0}\\min\\limits_{w,b}L\\left(w,b,\\alpha\\right)=d^*$$\n",
    "\n",
    "交换以后的新问题是原始问题的对偶问题，这个新问题的最优值用来$d^*$来表示。而且有$d^*≤p^*$，在满足某些条件的情况下，这两者相等，这个时候就可以通过求解对偶问题来间接地求解原始问题。  \n",
    "\n",
    "换言之，之所以从min max的原始问题$p^*$，转化为max min的对偶问题$d^*$，一者因为是近似解，二者，转化为对偶问题后，更容易求解。 下面可以先求L对w、b的极小，再求L对$\\alpha$的极大。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0GDW6Tp-pDwl"
   },
   "source": [
    "**五 对偶问题的求解**  \n",
    "\n",
    "$$L\\left(w,b,\\alpha\\right)=\\frac{1}{2}{\\Vert w\\Vert}^2-\\sum_{i=1}^{n}\\alpha_i{\\lgroup}y_i\\left(w^Tx_i+b\\right)-1\\rgroup$$\n",
    "\n",
    "首先固定$\\alpha$,要让L关于w和b最小化，我们分别对w,b求偏导数，即令：\n",
    "\n",
    "$$\\frac{\\partial{L}}{\\partial{w}}=0$$\n",
    "$$\\frac{\\partial{L}}{\\partial{b}}=0$$\n",
    "\n",
    "$$\\frac{\\partial{L}}{\\partial{w}}=0\\Rightarrow \\parallel w \\parallel =\\sum_{i=1}^{n}\\alpha_iy_ix_i$$\n",
    "$$\\frac{\\partial{L}}{\\partial{b}}=0\\Rightarrow\\sum_{i=1}^{n}\\alpha_iy_i=0$$\n",
    "将以上结果带入之前的$L\\left(w,b,\\alpha\\right)$：\n",
    "$$L(w,b,\\alpha)=\\frac{1}{2}{w}^2-\\sum_{i=1}^{n}\\alpha_i{\\lgroup}y_i\\left(w^Tx_i+b\\right)-1{\\rgroup}$$\n",
    "\n",
    "得到：\n",
    "$$L(w,b,\\alpha)=\\frac{1}{2}\\sum_{i,j=1}^{n}\\alpha_i\\alpha_jy_iy_j{x_i}^Tx_j-\\sum_{i,j=1}^{n}\\alpha_i\\alpha_jy_iy_j{x_i}^Tx_j-b\\sum_{i=1}^{n}\\alpha_iy_i+\\sum_{i=1}^{n}\\alpha_i=\\sum_{i=1}^{n}\\alpha_i-\\frac{1}{2}\\sum_{i,j=1}^{n}\\alpha_i\\alpha_j{y_iy_j{x_i}^Tx_j}$$\n",
    "\n",
    "求对$\\alpha$的极大，即是关于对偶问题的最优化问题。经过上面一个步骤的求w和b，得到的拉格朗日式子已经没有了变量w,b，只有$\\alpha$。从上面的式子得到：\n",
    "$$\\max\\limits_{\\alpha}\\sum_{i=1}^{n}\\alpha_i-\\frac{1}{2}\\sum_{i,j=1}^{n}\\alpha_i\\alpha_jy_iy_j{x_i}^Tx_j$$\n",
    "$$s.t.,\\alpha_i>=0,i=1,...,n$$\n",
    "$$\\sum_{i=1}^{n}\\alpha_iy_i=0$$\n",
    "\n",
    "这样，求出了$\\alpha_i$，根据:$$w=\\sum_{i=1}^{n}\\alpha_iy_ix_i$$\n",
    "即可求出w，然后通过\n",
    "$$b^*=-\\frac{\\max_{i:y^i = -1}w^Tx^i+\\min_{i:y^i = 1}w^Tx^i}{2}$$\n",
    "即可求出b，最终得出分离超平面和分类决策函数。  \n",
    "\n",
    "\n",
    "在求得$L(w,b,\\alpha)$关于w和b最小化，以及对$\\alpha$的极大之后，最后一步可以利用SMO算法求解对偶问题中的拉格朗日乘子$\\alpha$。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UPMXXwsZpDwn"
   },
   "source": [
    "**在我们刚开始讨论支持向量机时，我们就假定数据是线性可分的，也就是我们可以找到一个可行的超平面将数据完全分开。但是如果数据有噪音，而不是因为数据本身不是非线性结构。对于这种偏离正常位置很远的数据点，我们称之为outlier。**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DLUOyOd1pDwp"
   },
   "source": [
    "**五 使用松弛变量处理outliers方法**  \n",
    "\n",
    "在原先的SVM模型中，outlier的存在可能造成很大的影响，因为超平面本身就是只有少数几个support vector组成，如果这些support vector里又存在outlier的话，其影响就很大了。例如下图：\n",
    "![4.png](image/4.png)\n",
    "\n",
    "用黑圈圈起来的那个蓝点是一个 outlier ，它偏离了自己原本所应该在的那个半空间，如果直接忽略掉它的话，原来的分隔超平面还是挺好的，但是由于这个 outlier 的出现，导致分隔超平面不得不被挤歪了，变成途中黑色虚线所示（这只是一个示意图，并没有严格计算精确坐标），同时 margin 也相应变小了。当然，更严重的情况是，如果这个 outlier 再往右上移动一些距离的话，我们将无法构造出能将数据分开的超平面来。  \n",
    "\n",
    "为了处理这种情况，SVM 允许数据点在一定程度上偏离一下超平面。例如上图中，黑色实线所对应的距离，就是该 outlier 偏离的距离，如果把它移动回来，就刚好落在原来的超平面蓝色间隔边界上，而不会使得超平面发生变形了。  \n",
    "\n",
    "我们原来的约束条件变成了：\n",
    "$$y_i(w^Tx_i+b)>=1,i=1,...,n$$\n",
    "现在考虑outlier的问题，约束条件变成了：\n",
    "$$y_i(w^Tx_i+b)>=1-\\varepsilon_i,i=1,...n$$\n",
    "\n",
    "其中$\\varepsilon_i>=0$称为松弛变量，对应数据点$x_i$允许偏离的function margin的量。当然，如果我们运行$\\varepsilon_i$任意大的话，那任意的超平面都是符合条件的了。所以，我们在原来的目标函数后面加上一项，使得这些$\\varepsilon_i$的总和也要最下：\n",
    "$$\\min\\frac{1}{2}{\\parallel w\\parallel}^2+C\\sum_{i=1}^{n}\\varepsilon_i$$\n",
    "\n",
    "其中C是一个参数，用于控制目标函数中两项(“寻找margin最大的超平面和保证数据点偏差量最小”）之间的权重。注意，其中$\\varepsilon$是需要优化的变量之一，而C是一个事先确定好的常量。完整地写出来是这个样子：\n",
    "$$\\min\\frac{1}{2}{\\parallel w\\parallel}^2+C\\sum_{i=1}^{n}\\varepsilon_i$$\n",
    "$$s.t\\ y_i(w^Tx_i+b)>=1-\\varepsilon_i,i=1,...n$$\n",
    "$$\\varepsilon_i>=0,i=1,...,n$$\n",
    "\n",
    "用之前的方法将约束条件加入到目标函数中，得到的新的拉格朗日函数，如下所示：\n",
    "$$L(w,b,\\varepsilon,\\alpha,\\gamma)=\\frac{1}{2}{\\parallel w\\parallel}^2+C\\sum_{i=1}^{n}\\varepsilon_i-\\sum_{i=1}^{n}\\alpha_i{\\lgroup}y_i\\left(w^Tx_i+b\\right)-1+\\varepsilon_i\\rgroup-\\sum_{i=1}^{n}\\gamma_i\\varepsilon_i$$\n",
    "\n",
    "分析方法和前面一样，转换为另一个问题后，我们先让L针对$w,b和\\varepsilon$最小化：\n",
    "$$\\frac{{\\partial}L}{\\partial w}=0\\Rightarrow w=\\sum_{i=1}{n}\\alpha_iy_ix_i$$\n",
    "\n",
    "$$\\frac{{\\partial}L}{\\partial{b}}=0\\Rightarrow\\sum_{i=1}^{n}\\alpha_iy_i=0$$\n",
    "\n",
    "$$\\frac{{\\partial}L}{\\partial\\varepsilon_i}=0{\\Rightarrow}C-\\alpha_i-\\gamma_i=0,i=1,...,n$$\n",
    "将$w$带回L并化简，得到和原来一样的目标函数：\n",
    "$$\\max\\limits_{\\alpha}\\sum_{i=1}^{n}\\alpha_i-\\frac{1}{2}\\sum_{i,j=1}^{n}\\alpha_i\\alpha_jy_iy_j<x_i,x_j>$$\n",
    "$$s.t.\\  0<=\\alpha_i<=C,i=1,...,n$$\n",
    "$$\\sum_{i=1}^{n}\\alpha_iy_i=0$$\n",
    "可以看到唯一区别就是现在dual variable $\\alpha$多了一个上限C。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3WNvki-1pDwq"
   },
   "source": [
    "**写到这里，可以做个小结，不准确的说，SVM它本质上即是一个分类方法，用$w^Tx+b$定义分类函数，于是求w、b为寻找最大间隔。引出$\\frac{1}{2}{\\parallel w \\parallel}^2$,继而引出拉格朗日因子，转化为对拉格朗日乘子$\\alpha$的求解（求解过程会涉及一系列最优化或凸二次规划等问题），如此，求w、b与求$\\alpha$等价，而$\\alpha$的求解可以用一种快速学习算法SMO。**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Yrh_E5n_pDwq"
   },
   "source": [
    "**六 核函数** \n",
    "\n",
    "**在以上的问题中，我们都假设数据本身都是线性可分的，事实上，大部分时候数据并不是线性可分的，这个时候满足这样的条件的超平面就根本不存在。在前面，我们已经了解到SVM处理线性可分的情况，那对于非线性的数据SVM如何处理？这时候SVM的处理方法是选择一个核函数K。通过将数据映射到高纬空间，来解决在原始空间中线性不可分的问题。**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BowqeBx7pDwr"
   },
   "source": [
    "\n",
    "具体来说，在线性不可分的情况下，支持向量机首先在低维空间中完成计算，然后通过核函数将输入空间映射到高维特征空间，最终在高维特征空间中构造出最优分离超平面，从而把平面上本身不好分的非线性数据分开。如图所示，一维数据在二维空间无法划分，从而映射到三维空间里划分：\n",
    "![7.png](image/7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ob7MysdepDws"
   },
   "source": [
    "那么kernel是如何达到这个效果的呢？对于一个2维空间数据点v = (x, y)，要把它映射到3维空间，其中一种映射关系是：$p(x, y) = (x^2, \\sqrt 2xy, y^2)$。假如有任意两个数据点：$v_1=(x_1,y_1)$, $v_2=(x_2,y_2)$，我们可以直接计算它们对应向量的内积为：$$< p(v_1), p(v_2) > = < (x_1^2, \\sqrt 2x_1y_1, y_1^2), (x_2^2, \\sqrt 2x_2y_2, y_2^2) >$$<br>很明显，这是一个3维运算。假如3维还是无法线性区分，要映射到N维去，那这里就是N维运算，N越大计算量越大。有没有办法能够降低这个计算量呢？我们来展开推导一下：\n",
    "<br><br>$$< p(v_1), p(v_2) > = < (x_1^2, \\sqrt 2x_1y_1, y_1^2), (x_2^2, \\sqrt 2x_2y_2, y_2^2) > = (x_1x_2 + y_1y_2)^2 = (<v_1, v_2>)^2$$\n",
    "<br><br>\n",
    "经过推导可以看到，两个数据点在映射后的向量内积等于映射前向量内积的平方。如果我们引入核函数：$K(v_1, v_2) = (<v_1, v_2>)^2$，那么就可以通过核函数的计算来等价于映射后的向量内积，实现了高维向量运算转换为低维向量计算（本例中是2维向量运算代替了3维向量运算）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N7fc9GEupDws"
   },
   "source": [
    "核函数相当于把原来的分类函数：\n",
    "$$f(x)=\\sum_{i=1}^{n}\\alpha_iy_i<x_i,x>+b$$\n",
    "映射成：\n",
    "$$f(x)=\\sum_{i=1}^{n}\\alpha_iy_i<\\phi(x_i),\\phi(x)>+b$$\n",
    "而其中的$\\alpha$可以通过求解如下的dual问题而得到的：\n",
    "$$\\max\\limits_{\\alpha}\\sum_{i=1}^{n}{\\alpha_i}-\\frac{1}{2}\\sum_{i,j=1}^{n}{\\alpha_i}{\\alpha_j}y_iy_j<\\phi(x_i),\\phi(x_j)>$$\n",
    "$$s.t.,\\alpha_i>=0,i=1,...,n$$\n",
    "$$\\sum_{i=1}^{n}\\alpha_iy_i=0$$\n",
    "举个简单的例子，设两个向量$x_1={(\\eta_1,\\eta_2)}^T和x_2={(\\zeta_1.\\zeta_2)}^T，\\phi（）为映射关系。$映射后的内积为：\n",
    "$$<\\phi(x_1),\\phi(x_2)>=\\eta_1\\zeta_1+\\eta_1^2\\zeta_1^2+\\eta_2\\zeta_2+\\eta_2^2\\zeta_2^2+\\eta_1\\eta_2\\zeta_1\\zeta_2$$\n",
    "另外，我们注意到：\n",
    "$${(<x_1,x_2>+1)}^2=2\\eta_1\\zeta_1+\\eta_1^2\\zeta_1^2+2\\eta_2\\zeta_2+\\eta_2^2\\zeta_2^2+2\\eta_1\\eta_2\\zeta_1\\zeta_2+1$$\n",
    "二者有很多相似的地方，实际上，我们只需要把某几个维度线性缩放一下，然后再加上一个常数维度，具体来说，上面这个式子的计算结果实际上和映射$\\varphi(x_1,x_2)={(\\sqrt2x_1,x_1^2+\\sqrt2x_2,x_2^2,\\sqrt2x_1x_2,1)}^T$之后的内存$<\\varphi(x_1),\\varphi(x_2)>$的结果是相等的。区别在于一个是是映射到高维空间中，然后根据内积的公式进行计算；而另一个则是直接在原来的低维空间中进行计算，而不需要显示地写出映射后的结果。  \n",
    "\n",
    "我们把这里的计算两个向量在隐式映射过后的空间中的内积的函数叫做核函数，核函数能简化映射空间的内积运算。现在我们的分类函数为：\n",
    "$$f(x)=\\sum_{i=1}^{n}\\alpha_iy_iK<x_i,x>+b$$\n",
    "而其中的$\\alpha$可以通过求解如下的dual问题而得到的：\n",
    "$$\\max\\limits_{\\alpha}\\sum_{i=1}^{n}{\\alpha_i}-\\frac{1}{2}\\sum_{i,j=1}^{n}{\\alpha_i}{\\alpha_j}y_iy_jK(x_i,x_j)$$\n",
    "$$s.t.,\\alpha_i>=0,i=1,...,n$$\n",
    "$$\\sum_{i=1}^{n}\\alpha_iy_i=0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S1nogf__pDwt"
   },
   "source": [
    "**七 SMO算法**  \n",
    "\n",
    "在上面我们得到了最后的目标函数：\n",
    "$$\\max\\limits_{\\alpha}\\sum_{i=1}^{n}\\alpha_i-\\frac{1}{2}\\sum_{i,j=1}^{n}\\alpha_i\\alpha_jy_iy_j<x_i,x_j>$$\n",
    "$$s.t.,0<=\\alpha_i<=C,i=1,...,n$$\n",
    "$$\\sum_{i=1}^{n}\\alpha_iy_i=0$$\n",
    "\n",
    "等价于求：\n",
    "$$\\min\\limits_\\alpha\\Psi\\left(\\vec{\\alpha}\\right)=\\min\\limits_{\\alpha}\\frac{1}{2}\\sum_{i=1}^{n}\\sum_{j=1}^{n}y_iy_jK\\left<x_i,x_j\\right>\\alpha_i\\alpha_j-\\sum_{i=1}^{n}\\alpha_i$$\n",
    "$$0<=\\alpha_i<=C,i=1,...,n$$\n",
    "$$\\sum_{i=1}^{n}y_i\\alpha_i=0$$\n",
    "**这里得到的min函数与我们之前的max函数实质是一样的。**    \n",
    "\n",
    "下面要解决的问题是：在$\\vec{\\alpha}={\\alpha_1,\\alpha_2,...\\alpha_n}$上求解目标函数的最小值。为了求解这些乘子，每次从中任意抽取两个乘子$\\alpha_1$和$\\alpha_2$，然后固定$\\alpha_1$和$\\alpha_2$以外的其它乘子${\\alpha_3,...,\\alpha_n}$，使得目标函数只是关于$\\alpha_1$和$\\alpha_2$的函数。这样，不断的从一堆乘子中任意抽取两个求解，不断迭代求解子问题，最终达到求解原问题的目的。\n",
    "而原对偶问题的子问题的目标函数可以表达为：\n",
    "$$\\Psi=\\frac{1}{2}K_{11}{\\alpha_1}^2+\\frac{1}{2}K_{22}{\\alpha_2}^2+sK_{12}\\alpha_1\\alpha_2+y_1{\\alpha}_1\\nu_1+y_2\\alpha_2\\nu_2-\\alpha_1-\\alpha_2+\\Psi_{constant}$$\n",
    "其中:\n",
    "$$K_{ij}=K(x_i,x_j)$$\n",
    "$$\\nu_i=\\sum_{j=3}^{n}y_j{\\alpha_j}^*K_{ij}=u_i+b^*-y_1{\\alpha_1}^*K_{1i}-y_2{\\alpha_2}^*K_{2i}$$\n",
    "$$u_i=w^Tx_i+b$$\n",
    "\n",
    "为了解决这个子问题，首要问题便是每次如何选取$\\alpha_1$和$\\alpha_2$。实际上，其中一个乘子是违反KKT条件最严重的，另外一个乘子则由另一个约束条件选取。  \n",
    "根据KKT条件可以得出目标函数中$\\alpha_i$取值的意义：\n",
    "$$\\alpha_i=0{\\Leftrightarrow}y_iu_i>=1$$\n",
    "$$0<\\alpha_i<C{\\Leftrightarrow}y_iu_i=1$$\n",
    "$$\\alpha_i=C{\\Leftrightarrow}y_iu_i<=1$$\n",
    "这里的$\\alpha_i$还是拉格朗日乘子：  \n",
    "1.对于第一种情况，表明$\\alpha_i$是正常分类，在间隔边界内部（我们知道正确分类的点$y_i*f(x_i)>=0$）  \n",
    "2.对于第二种情况，表明了$\\alpha_i$是支持向量，在间隔边界上；  \n",
    "3.对于第三种情况，表明了$\\alpha_i$是在两条间隔边界之间；  \n",
    "\n",
    "而最优解需要满足KKT条件，既满足3个条件都得满足，以下几种情况出现将会出现不满足：  \n",
    "$y_iu_i$<=1但是$\\alpha_i$<C则是不满足的,而原本$\\alpha_i$<C则是不满足的，而原本$\\alpha_i$=C  \n",
    "$y_iu_i$>=1但是$\\alpha_i$>0则是不满的，而原本$\\alpha_i$=0  \n",
    "$y_iu_i$=1但是$\\alpha_i$=0或者$\\alpha_i$=C则表明不满足，而原本应该是0<$\\alpha_i$<C\n",
    "\n",
    "也就是说，如果存在不满足KKT条件的$\\alpha_i$，那么需要更新这些$\\alpha_i$，这是第一个约束条件。此外，更新的同时还要受到第二个约束条件的限制，即：\n",
    "$$\\sum_{i=1}^{n}y_i\\alpha_i=0$$\n",
    "因此，如果假设选择的两个乘子$\\alpha_1$和$\\alpha_2$，它们在更新之前分别是${\\alpha_1}^{old}$、${\\alpha_2}^{old}$,更新之后分别是${\\alpha_1}^{new}$、${\\alpha_2}^{new}$,那么更新前后的值需要满足以下等式才能保证和为0的约束：\n",
    "$${\\alpha_1}^{new}y_1+{\\alpha_2}^{new}y_2={\\alpha_1}^{old}y_1+{\\alpha_2}^{old}y_2=\\zeta$$\n",
    "其中$\\zeta$是常数。\n",
    "两个因子不好同时求解，所以可先求第二个乘子$\\alpha_2$的解（${\\alpha_2}^{new}$）,得到$\\alpha_2$的解之后，再利用$\\alpha_2$的解表示$\\alpha_1$的解（${\\alpha_1}^{new}$）  \n",
    "为了求解${\\alpha_2}^{new}$,得先确定${\\alpha_2}^{new}$取值范围。假设它的上下边界分别为H和L，那么有：\n",
    "$$L<={\\alpha_2}^{new}<=H$$\n",
    "接下来，综合$0<=\\alpha_i<=C,i=1,...,n和{\\alpha_1}^{new}y_1+{\\alpha_2}^{new}y_2={\\alpha_1}^{old}y_1+{\\alpha_2}^{old}y_=\\zeta$这两个约束条件，求取${\\alpha_2}^{new}$值范围。  \n",
    "当$y_1!=y_2$，根据${\\alpha_1}^{new}y_1+{\\alpha_2}^{new}y_2={\\alpha_1}^{old}y_1+{\\alpha_2}^{old}y_=\\zeta$可得${\\alpha_1}^{old}-{\\alpha_2}^{old}=\\zeta$,所以$L=\\max(0,-\\zeta),H=\\min(C,C-\\zeta)$如下图所示：\n",
    "![5.png](image/5.png)\n",
    "当$y_1=y_2$时，同样根据${\\alpha_1}^{new}y_1+{\\alpha_2}^{new}y_2={\\alpha_1}^{old}y_1+{\\alpha_2}^{old}y_=\\zeta$可得：${\\alpha_1}^{old}+{\\alpha_2}^{old}=\\zeta$,所以有$L=\\max(0,\\zeta-C),H=\\min(C,\\zeta)$,如下图所示：\n",
    "![6.png](image/6.png)\n",
    "如此，根据$y_1$和$y_2$异号或同号，可得出${\\alpha_2}^{new}$的上下界分别为：\n",
    "$$L=\\max(0,-\\zeta),H=\\min(C,C-\\zeta)  ify_1 != y_2$$\n",
    "$$L=\\max(0,\\zeta-C),H=\\min(C,\\zeta)   if y_1 = y_2$$\n",
    "\n",
    "回顾下第二个约束条件${\\alpha_1}^{new}y_1+{\\alpha_2}^{new}y_2={\\alpha_1}^{old}y_1+{\\alpha_2}^{old}y_=\\zeta$,令上式两边分别乘以$y_1$，可得$\\alpha_1+s\\alpha_2={\\alpha_1}^*+s{\\alpha_2}^*=w$,其中:\n",
    "$$w=-y_1\\sum_{i=3}^{n}y_i{\\alpha_i}^*$$\n",
    "因此$\\alpha_1$可以用$\\alpha_2$表示，$\\alpha_1=w-s\\alpha_2$,从而把子问题的目标函数转换为只含$\\alpha_2$的问题：\n",
    "$$\\Psi=\\frac{1}{2}K_{11}{(w-s\\alpha_2)}^2+\\frac{1}{2}K_{22}{\\alpha_2}^2+sK_{12}(w-s\\alpha2)\\alpha_2+y_1(w-s\\alpha_2)\\nu_1-w+s\\alpha_2+y_2\\alpha_2\\nu_2-\\alpha_2+\\Psi_{constant}$$\n",
    "对$\\alpha_2$求导可得：\n",
    "$$\\frac{\\mathrm{d}\\Psi}{\\mathrm{d}\\alpha_2}=-sK_{11}(w-s\\alpha_2)+K_{22}\\alpha_2-K_{12}\\alpha_2+sK_{12}(w-s\\alpha_2)-y_2\\nu_1+s+y_2\\nu_2-1=0$$\n",
    "\n",
    "化简如下:\n",
    "$$\\alpha2(K_{11}+K_{22}-2K_{11})=s(K_{11}-K_{12})w+y_2(\\nu_1-\\nu_2)+1-s$$\n",
    "$$K_{ij}=K(x_i,x_j)$$\n",
    "然后将$s=y_1*y_2$、$\\alpha_1+s\\alpha_2={\\alpha_1}^*+s{\\alpha_2}^*=w$和$\\nu_i=\\sum_{j=3}^{n}y_j{\\alpha_j}^*K_{ij}=u_i+b^*-y_1{\\alpha_1}^*K_{1i}-y_2{\\alpha_2}^*K_{2i}$代入上式可得：\n",
    "$${\\alpha_2}^{new,wnc}(K_{11}+K_{22}-2K_{12})={\\alpha_2}^{old}(K_{11}+K_{22}-2K_{12})+y_2(u_1-u_2+y_2-y_1)$$\n",
    "\n",
    "令$E_i=u_i-y_i$(表示预测值与真实值之差),$\\eta=K<x_1,x_1)+K(x_2,x_2)-2K(x_1,x_2)$上式两边同时除以$\\eta$，得到一个关于单变量$\\alpha_2$的解：\n",
    "$${\\alpha_2}^{new,wnc}={\\alpha_2}^{old}+\\frac{y_2(E_1-E_2)}{\\eta}$$\n",
    "\n",
    "这个解没有考虑约束条件$0<=\\alpha_2<=C$。考虑约束条件经过剪辑$\\alpha_2^{new}$的解为：  \n",
    "1  ${\\alpha_2}^{new,wnc}>H$\n",
    "$${\\alpha_2}^{new}=H$$\n",
    "2  $L<={\\alpha_2}^{new,wnc}<=H$\n",
    "$${\\alpha_2}^{new}={\\alpha_2}^{new,wnc}$$\n",
    "3  ${\\alpha_2}^{new,wnc}<L$\n",
    "$${\\alpha_2}^{new}=L$$\n",
    "求出了${\\alpha_2}^{new}后，便可求出\\alpha_1^{new}, 得\\alpha_1^{new}=\\alpha_1^{old}+y_1y_2(\\alpha_2^{old}-\\alpha_2^{new})$  \n",
    "如何选择乘子$\\alpha_1和\\alpha_2$呢？  \n",
    "对于$\\alpha_1,即第一个乘子,可以通过刚刚说的那三种不满足KKT的条件来找$；\n",
    "而对于第二个乘子$\\alpha_2可以寻找满足条件:\\max|E_i-Ej|的乘子。$  \n",
    "而b在满足下述条件下更新b：\n",
    "\n",
    "1  $0<\\alpha_1^{new}<C$\n",
    "$$b=b_1$$\n",
    "2  $0<\\alpha_2{new}<C$\n",
    "$$b=b_2$$\n",
    "3  $otherwise$\n",
    "$$b=\\frac{b_1+b_2}{2}$$\n",
    "\n",
    "$$b_1^{new}=b^{old}-E_1-y_1(\\alpha_1^{new}-\\alpha_1^{old}k(x_1,x_1)-y_2(\\alpha_2^{new}-\\alpha_2^{old})k(x_1,x_2)$$\n",
    "\n",
    "$$b_2^{new}=b^{old}-y_1(\\alpha_1^{new}-\\alpha_1^{old})k(x_1,x_2)-y_2(\\alpha_2^{new}-\\alpha_2^{old})k(x_2,x_2)-E_2$$\n",
    "且每次更新完两个乘子的优化后,都需要再重新计算b,以及对应的$E_i$值。  \n",
    "最后更新所有的$\\alpha_i,y和b,这样模型就训练出来了,从而即可求出我们开头提出的分类函数:$  \n",
    "$$f(x)=\\sum_{i=1}^{n}\\alpha_iy_i<x_i,x>+b$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lG2bWxpBpDwu"
   },
   "source": [
    "**现在我们来总结下SMO算法的步骤**  \n",
    "1 第一步选取一对$\\alpha_i和\\alpha_j，选取方法使用启发式方法$。  \n",
    "2 第二步，固定$除\\alpha_i和\\alpha_j之外的其它参数,确定W极值条件下的\\alpha_i,\\alpha_j由\\alpha_i表示。$  \n",
    "3 第三步，更新乘子，选取乘子的方式如下：\n",
    "$$先扫描所有乘子，把第一个违反KKT条件的作为更新对象，令为\\alpha_1$$\n",
    "$在所有不违反KKT条件的乘子中，选择使|E_1-E_2|最大的\\alpha_2进行更新，使得能最大限度增大目标函数的值。$  \n",
    "\n",
    "4 最后，每次更新完两个乘子的优化后，都需要再重新计算b,以及对应的$E_i$值。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CK-D0y0WpDwu"
   },
   "source": [
    "**经过前面几节的铺垫，我们了解SVM对偶形式的求解，现在我们就来用代码实现SVM。**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yLfY5dGGpDwv"
   },
   "source": [
    "**任务一 从DataSet.txt中导入数据，获得训练集以及标签。**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0aPiRpNPpDwv"
   },
   "source": [
    "定义函数LoadData(filename),参数为文件名，返回训练数据集以及标签。数据集由两个特征$X_1和X_2$构成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "15Vs1R_npDww"
   },
   "outputs": [],
   "source": [
    "#导入相应的库包\n",
    "from numpy import *\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)  #消除警告"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 137
    },
    "colab_type": "code",
    "id": "-Kzw5gSVpDw1",
    "outputId": "491c405b-be51-4910-f359-cf3d3d946f32",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def LoadData(filename):\n",
    "    data = []\n",
    "    label = []\n",
    "    with open(filename) as f:                               \n",
    "        for line in f.readlines():     #按行读取\n",
    "            ### START THE CODE ###\n",
    "            \n",
    "                                                          #将特征存放到Data中\n",
    "                                                         #将标签存放到Label中\n",
    "            ### END THE CODE ###\n",
    "    return data,label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BtUABdNjpDw9"
   },
   "source": [
    "获取训练集，数据和标签"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o_x1F7IGpDw-"
   },
   "outputs": [],
   "source": [
    "TrainData, TrainLabel = LoadData('DataSet.txt')\n",
    "print (\"TrainData = \",TrainData[:3])\n",
    "print (\"TrainLabel = \",TrainLabel[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6ZyvZj2XpDxC"
   },
   "source": [
    "输出：\n",
    "\n",
    "TrainData = [[3.542485, 1.977398], [3.018896, 2.556416], [7.55151, -1.58003]]\n",
    "\n",
    "TrainLabel = [-1.0, -1.0, 1.0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NrEF5RNepDxD"
   },
   "source": [
    "**定义所需操作的数据结构DataOp如下**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AiHiHOKnpDxD"
   },
   "outputs": [],
   "source": [
    "class DataOp(object):\n",
    "    def __init__(self,data,label,C,toler):      #定义构造函数\n",
    "        self.X = data                           #数据\n",
    "        self.label = label                      #标签\n",
    "        self.C = C                              #松弛变量\n",
    "        self.tol = toler                        #容忍度\n",
    "        self.m = shape(data)[0]                 #特征向量的第一个维度\n",
    "        self.alpha = mat(zeros((self.m,1)))     #Alpha个数初始化\n",
    "        self.b = 0                              #步长\n",
    "        self.Cache = mat(zeros((self.m,2)))     #第一列给出是否有效 第二列给出的是实际的E值       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QaZgRDvKpDxI"
   },
   "source": [
    "**我们利用上述定义的数据结构来表达我们整个训练模型的过程需要的参数以及数据，初始化一个DataOp对象**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3ZEEaVB4pDxJ"
   },
   "outputs": [],
   "source": [
    "oS = DataOp(mat(TrainData), mat(TrainLabel).transpose(), 0.6, 0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O2LKy9dGpDxN"
   },
   "source": [
    "**在选择乘子过程中，我们需要选中两个不同的乘子，所以当我们固定了其中一个乘子，就需要选出不同于第一个乘子的第二个乘子。我们定义一个函数SelectAlpha来实现这个过程。**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u7G4AZ5wpDxO"
   },
   "source": [
    "函数：SelectAlpha(i,m)  \n",
    "作用：随机选择一个整数j,(0<=j<=m-1 && j!=i)  \n",
    "参数：i第一个乘子的下标，m所有alpha的个数  \n",
    "返回：一个随机选择不同于i的下标j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZlZFcerwpDxO"
   },
   "outputs": [],
   "source": [
    "def  SelectAlpha(i,m):\n",
    "    j = i\n",
    "    while (j == i):\n",
    "        j = int(random.uniform(0,m))  \n",
    "    return j"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XRCChWvOpDxS"
   },
   "source": [
    "**任务二 调整alpha的值**    \n",
    "\n",
    "根据如下规则来对我们计算出的alpha进行调整。  \n",
    "1  ${\\alpha_2}^{new,wnc}>H$\n",
    "$${\\alpha_2}^{new}=H$$\n",
    "2  $L<={\\alpha_2}^{new,wnc}<=H$\n",
    "$${\\alpha_2}^{new}={\\alpha_2}^{new,wnc}$$\n",
    "3  ${\\alpha_2}^{new,wnc}<L$\n",
    "$${\\alpha_2}^{new}=L$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lM-zO8kUpDxV"
   },
   "source": [
    "函数：ResetAlpha(Alphaj,low,high)\n",
    "\n",
    "作用：调整Alphaj(即$\\alpha_j$)的值，使得low<=Alphaj<=high，调整幅度尽可能小\n",
    "\n",
    "参数：Alphaj 目标值， low 最小值， high最大值\n",
    "\n",
    "返回：调整后的Alphaj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OZJsY3hnpDxW"
   },
   "outputs": [],
   "source": [
    "def ResetAlpha(Alphaj,low,high):\n",
    "    ### STARD CODE HERE ###\n",
    "    \n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    return Alphaj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aV4xTp6hpDxc"
   },
   "outputs": [],
   "source": [
    "a = 10\n",
    "b = ResetAlpha(a,11,20)\n",
    "c = ResetAlpha(a,1,8)\n",
    "print(\"b = \", b)\n",
    "print(\"c = \", c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VOYFSGB_pDxf"
   },
   "source": [
    "输出：  \n",
    "b =  11  \n",
    "c =  8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cSzXVhp0pDxf"
   },
   "source": [
    "**任务三 上述原理过程中，需要计算真实值与预测值之间的误差,定义一个函数ComputeEk。**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PGz3rdNhpDxg"
   },
   "source": [
    "函数 ComputeEk(os,k)\n",
    "\n",
    "作用：求Ek误差 = 预测值 - 真实值。\n",
    "真实值即样本标签，以下公式计算预测值$f(x)=\\sum_{i=1}^{n}\\alpha_iy_i<x_i,x>+b$\n",
    "\n",
    "参数：os DataOp对象，k 具体的某一行\n",
    "\n",
    "返回：预测值与真实结果对比，计算误差Ek  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y9lyiqmHpDxh"
   },
   "outputs": [],
   "source": [
    "def ComputeEk(os,k):\n",
    "    PredictK = float(multiply(os.alpha,os.label).T * (os.X*os.X[k,:].T)) + os.b\n",
    "    ### START CODE HERE ###\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    return Ek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VJQLMOWjpDxk"
   },
   "outputs": [],
   "source": [
    "Ek1 = ComputeEk(oS,25)\n",
    "Ek2 = ComputeEk(oS,30)\n",
    "print (\"Ek1 = \", Ek1)\n",
    "print (\"Ek2 = \", Ek2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T_Ya1E9qpDxn"
   },
   "source": [
    "输出：\n",
    "\n",
    "Ek1 = -1.0\n",
    "\n",
    "Ek2 = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7CDrjx_epDxo"
   },
   "source": [
    "**任务四 选取最大$|E_i-E_j|$最大的j，并返回j以及$E_j$**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cFfpDHdopDxq"
   },
   "source": [
    "函数：SelectMaxJ(i,oS,Ei)    \n",
    "\n",
    "作用：返回最有的j和Ej，选择第二个（内循环）值以保证每次优化中采用最大步长。\n",
    "\n",
    "这里的目标是选择合适的第二个alpha值以保证每次优化中采用最大步长\n",
    "\n",
    "该函数的误差与第一个alpha值Ei和下标i有关。\n",
    "\n",
    "参数：\n",
    "\n",
    "i    具体的第i行\n",
    "\n",
    "oS   DataOp对象\n",
    "\n",
    "Ei   预测值与真实值对比，计算Ei\n",
    "\n",
    "返回：\n",
    "\n",
    "j    随机选出的第j行\n",
    "\n",
    "Ej   预测结果与真实值对比，计算误差Ej"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W-bHozLNpDxr"
   },
   "outputs": [],
   "source": [
    "def SelectMaxj(i,oS,Ei):\n",
    "    MaxK = -1              #保存最大下标值\n",
    "    MaxDeltaE = 0          #保存最大步长\n",
    "    Ej = 0\n",
    "    oS.Cache[i] = [1,Ei]   #首先将输入值Ei在缓存中设置为有效的。这里意味着它已经计算好了\n",
    "    List = nonzero(oS.Cache[:,0].A)[0]\n",
    "    if (len(List)) > 1:               # 在所有的值上进行循环，并选择使得改变最大的那个值\n",
    "        for k in List:\n",
    "            if k == i:                \n",
    "                continue             #不计算\n",
    "            ### START CODE HERE ###\n",
    "\n",
    "                                             # 计算DeltaE\n",
    "                                            #DeltaE > MaxDeltaE , 则进行更新\n",
    "                                              #更新下标\n",
    "                                            #最大值更新\n",
    "                                            #替换Ej\n",
    "            ### END CODE HERE ###\n",
    "        return MaxK, Ej\n",
    "    else:                                #如果是第一次循环，则随机选择一个alpha\n",
    "        j = SelectAlpha(i,oS.m)    \n",
    "        Ej = ComputeEk(oS,j)\n",
    "    return j,Ej"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ao9--XZQpDxu"
   },
   "outputs": [],
   "source": [
    "Data =  [[3.542485, 1.977398], [3.018896, 2.556416], [7.55151, -1.58003], [2.114999, -0.004466], [8.127113, 1.274372]]\n",
    "Label =  [-1.0, -1.0, 1.0, -1.0, 1.0]\n",
    "TestOs = DataOp(mat(Data),mat(Label).transpose(),0.6,0.001)\n",
    "TestEi = ComputeEk(TestOs,0)\n",
    "TestOs.Cache[1] = [1,ComputeEk(TestOs,1)]\n",
    "TestOs.Cache[2] = [1,ComputeEk(TestOs,2)]\n",
    "TestOs.Cache[3] = [1,ComputeEk(TestOs,3)]\n",
    "TestOs.Cache[4] = [1,ComputeEk(TestOs,4)]\n",
    "Testj,TestEj = SelectMaxj(0,TestOs,TestEi)\n",
    "\n",
    "print (\"Testj = \",Testj)\n",
    "print (\"TestEj = \", TestEj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-hRyu9NtpDxy"
   },
   "source": [
    "输出：\n",
    "\n",
    "j = 2\n",
    "\n",
    "Ej = -1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3_DzjQLdpDxy"
   },
   "source": [
    "**任务五 计算误差值并存入缓存，在对alpha值进行优化之后会用到这个函数**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z8FHqi7spDxz"
   },
   "source": [
    "函数：updateEk(oS,k)\n",
    "\n",
    "作用：计算误差值并存入缓存os.Cache，在对alpha值进行优化之后会用到这个函数\n",
    "\n",
    "参数：\n",
    "\n",
    "Os DataOpt对象\n",
    "\n",
    "k 某一列的行号\n",
    "\n",
    "返回：无\n",
    "\n",
    "例如某行为oS.Cache[k] = [1,Ek] 其中1表示有效。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Mra8qnZrpDx0"
   },
   "outputs": [],
   "source": [
    "def updataEk(oS,k):\n",
    "    ###START THE CODE ###\n",
    "                                                           #计算Ek\n",
    "                                                           #更新第k行的oS.Cache[k]\n",
    "    ###END THE CODE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Nz17-TWjpDx2"
   },
   "outputs": [],
   "source": [
    "TestOs = DataOp(mat(Data),mat(Label).transpose(),0.6,0.001)\n",
    "updataEk(TestOs,0)\n",
    "updataEk(TestOs,1)\n",
    "updataEk(TestOs,2)\n",
    "print (\"TestOs.Cache[0] = \",TestOs.Cache[0])\n",
    "print (\"TestOs.Cache[1] = \",TestOs.Cache[1])\n",
    "print (\"TestOs.Cache[2] = \",TestOs.Cache[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5SZyuzQMpDx6"
   },
   "source": [
    "输出： \n",
    "\n",
    "TestOs.Cache[0] =  [[1. 1.]]  \n",
    "\n",
    "TestOs.Cache[1] =  [[1. 1.]]  \n",
    "\n",
    "TestOs.Cache[2] =  [[1. -1.]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "og4COhRSpDx7"
   },
   "source": [
    "**SMO算法是通过一个外循环来选择第一个alpha值得，并且其选择过程会在两种方式之间交替：一种方式是在所有数据集上进行单遍扫描，另一种方式则是在非边界alpha中实现单遍扫描。而所谓非边界alpha指的就是那些不等于边界0或C的alpha的值。对整个数据集的扫描相当容易，而实现非边界alpha值得扫描时，首先需要建立这些alpha值的列表，然后再对这个表进行遍历。同时，该步骤会跳过那么已知的不会改变的alpha的值。\n",
    "在选择第一个alpha值后，算法会通过一个内循环来选择第二个alpha值。在优化过程中，会通过最大步长的方式来获得第二个alpha值。我们建立一个全局的缓存用于保存误差值，并从中选择使得步长最大的alpha值（Ei-Ej）**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DsD7w0p4pDx7"
   },
   "source": [
    "首先我们来看实现内循环的代码，如何选择另外第二个alpha乘子。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CDw4XlZRpDx8"
   },
   "source": [
    "函数：InsideCycle(i,oS)  \n",
    "\n",
    "作用：SMO算法内循环选择第二个alpha值\n",
    "\n",
    "参数：\n",
    "\n",
    "i    具体某一行\n",
    "\n",
    "oS   DataOp对象\n",
    "\n",
    "返回：\n",
    "\n",
    "0    找不到最优值\n",
    "\n",
    "1    找到了最优值，并且存储到oS.Cache中\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RI7OmGBGpDx9"
   },
   "outputs": [],
   "source": [
    "def InsideCycle(i,oS):\n",
    "    Ei = ComputeEk(oS,i)   #求Ek误差\n",
    "    #约束条件（KKT条件是解决最优化问题时用到的一种方法。我们这里提到的最优化问题通常\n",
    "    #是指对于给定的某一函数，求其在指定作用域上的全局最小值\n",
    "    #0<=alpha[i]<=C,但由于0和C是边界值，我们无法进行优化，因为需要升高一个alpha和降低一个alpha。\n",
    "    #表示发生错误的概率：label[i]*Ei ,如果超出toler，才需要优化。至于正负号，考虑绝对值就行\n",
    "    \n",
    "    #检验训练样本(xi,yi)是否满足KKT条件\n",
    "    #yi*f(xi) >= 1 and alpha = 0 (outside the boundary)\n",
    "    #yi*f(xi) == 1 0<alpha<C     (on the boundary)\n",
    "    #yi*f(xi) <= 1 and alpha = C  (between the boundary)\n",
    "    \n",
    "    if ((oS.label[i] * Ei < -oS.tol) and (oS.alpha[i] < oS.C)) or ((oS.label[i] * Ei > oS.tol) and (oS.alpha[i] > 0)):\n",
    "        #选择最大的误差对应的j进行优化。\n",
    "        j,Ej = SelectMaxj(i,oS,Ei)\n",
    "        IOldAlpha = oS.alpha[i].copy()\n",
    "        JOldAlpha = oS.alpha[j].copy()\n",
    "        \n",
    "        #L 和 H将用于将alpha[j]调整到0-C之间。如果L == H，就不做任何改变，直接Return 0\n",
    "        if (oS.label[i] != oS.label[j]):\n",
    "            L = max(0,oS.alpha[j] - oS.alpha[i])\n",
    "            H = min(oS.C, oS.C + oS.alpha[j] - oS.alpha[i])\n",
    "        else:\n",
    "            L = max(0,oS.alpha[j] + oS.alpha[i] - oS.C)\n",
    "            H = min(oS.C, oS.alpha[j] + oS.alpha[i])\n",
    "        \n",
    "        if L == H:\n",
    "            #print (\"L == H\")\n",
    "            return 0\n",
    "        \n",
    "        #eva 是alpha[j]的最优修改量，如果eva==0，需要退出for循环当前迭代过程\n",
    "        eva = 2.0 * oS.X[i, :] * oS.X[j, :].T - oS.X[i, :] * oS.X[i, :].T - oS.X[j, :] * oS.X[j, :].T\n",
    "        if eva >= 0:\n",
    "            #print(\"eva >= 0\")\n",
    "            return 0\n",
    "        \n",
    "        #计算一个新的alpha[j]值\n",
    "        oS.alpha[j] -= oS.label[j] * (Ei-Ej) / eva\n",
    "        #并使用辅助函数，以及L和H对其进行调整\n",
    "        oS.alpha[j] = ResetAlpha(oS.alpha[j],L,H)\n",
    "        #更新缓存误差\n",
    "        updataEk(oS,j)\n",
    "        \n",
    "        #检查alpha[j]是否只是轻微的改变，如果是的话，就退出for循环\n",
    "        if (abs(oS.alpha[j] - JOldAlpha) < 0.00001):\n",
    "            return 0\n",
    "        \n",
    "        #然后alpha[i]和alpha[j]做同样的修改，虽然改变的大小一样，但是改变的方向相反\n",
    "        oS.alpha[i] += oS.label[j] * oS.label[i] * (JOldAlpha - oS.alpha[j])\n",
    "        #更新误差缓存\n",
    "        updataEk(oS,i)\n",
    "        \n",
    "        #在对alpha[i],alpha[j]进行优化之后，给这个两个alpha值设置一个常数b。\n",
    "        '''\n",
    "         w= Σ[1~n] ai*yi*xi => b = yj Σ[1~n] ai*yi(xi*xj)\n",
    "         所以：  b1 - b = (y1-y) - Σ[1~n] yi*(a1-a)*(xi*x1)\n",
    "         为什么减2遍？ 因为是 减去Σ[1~n]，正好2个变量i和j，所以减2遍\n",
    "        '''\n",
    "        b1 = oS.b - Ei - oS.label[i] * (oS.alpha[i] - IOldAlpha) * oS.X[i, :] * oS.X[i, :].T - oS.label[j] * (oS.alpha[j] - JOldAlpha) * oS.X[i, :] * oS.X[j, :].T\n",
    "        b2 = oS.b - Ej - oS.label[i] * (oS.alpha[i] - IOldAlpha) * oS.X[i, :] * oS.X[j, :].T - oS.label[j] * (oS.alpha[j] - JOldAlpha) * oS.X[j, :] * oS.X[j, :].T\n",
    "        if (0 < oS.alpha[i]) and (oS.C > oS.alpha[i]):\n",
    "            oS.b = b1\n",
    "        elif (0 < oS.alpha[j]) and (oS.C > oS.alpha[j]):\n",
    "            oS.b = b2\n",
    "        else:\n",
    "            oS.b = (b1+b2) / 2.0\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P1kK06YCpDyA"
   },
   "source": [
    "**接下来我们实现SMO算法的外循环，外循环的结束迭代条件是：迭代次数达到最大迭代次数 或者 循环遍历所有alpha后，没有alpha改变。**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XJr-JCaNpDyB"
   },
   "source": [
    "函数：Smo(oS,IterStep）\n",
    "\n",
    "作用：SMO算法外循环，计算出拉格朗日乘子以及模型的常量b\n",
    "\n",
    "参数：\n",
    "\n",
    "oS DataOp对象  \n",
    "\n",
    "IterStep 退出前的最大循环次数\n",
    "\n",
    "返回：\n",
    "\n",
    "b       模型的常量值\n",
    "\n",
    "alpha   拉格朗日乘子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "megAwOczpDyC"
   },
   "outputs": [],
   "source": [
    "def Smo(oS,IterStep):\n",
    "    iter = 0                #迭代次数\n",
    "    EntireSet = True         #是否遍历了没有遍历整个alpha值\n",
    "    AlphaChanged = 0          #alpha改变的次数\n",
    "    \n",
    "    #循环迭代结束 或者 循环遍历所有alpha后，AlphaChanged还是没变化\n",
    "    \n",
    "    while (iter < IterStep) and ((AlphaChanged > 0) or (EntireSet)):\n",
    "        AlphaChanged = 0\n",
    "        #当EntireSet = True or 非边界alpha对没有了；就开始寻找alpha对，然后决定是否else。\n",
    "        if EntireSet:\n",
    "            #在数据集上遍历所有可能的alpha\n",
    "            for i in range(oS.m):\n",
    "                #是否存在alpha对，存在就+1\n",
    "                AlphaChanged += InsideCycle(i,oS)\n",
    "            iter += 1\n",
    "        #对已存在alpha对，选出非边界的alpha值，进行优化。\n",
    "        else:\n",
    "            #遍历所有非边界alpha值，进行优化。\n",
    "            nonBoundIs = nonzero((oS.alpha.A > 0) * (oS.alpha.A < oS.C))[0]\n",
    "            for i in nonBoundIs:\n",
    "                AlphaChanged += InsideCycle(i,oS)\n",
    "               \n",
    "            iter += 1\n",
    "        \n",
    "        #如果找到alpha对，就优化非边界alpha值，否则，就重新进行寻找，如果寻找一遍 遍历所有的行还是没找到，就退出循环。\n",
    "        if EntireSet:\n",
    "            EntireSet = False\n",
    "        elif (AlphaChanged == 0):\n",
    "            EntireSet = True\n",
    "    return oS.b, oS.alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RSe4CmadpDyG"
   },
   "source": [
    "**下面调用SMO算法，计算常量b以及a拉格朗日乘子alpha[50:55]。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JndPv9-cpDyH"
   },
   "outputs": [],
   "source": [
    "b, alphas = Smo(oS, 40)\n",
    "print (\"b = \", b)\n",
    "print (\"alphas = \",alphas[50:55])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UGUJTWFepDyL"
   },
   "source": [
    "**任务六 根据计算出的拉格朗日乘子计算出权重向量W，计算公式如下：**\n",
    "$$W=\\sum_{i=1}^{n}\\alpha_iy_ix_i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LroXTRNgpDyM"
   },
   "source": [
    "函数：ComputeW(alphas,data,label)\n",
    "\n",
    "作用：基于alphas计算W\n",
    "\n",
    "参数：\n",
    "\n",
    "alphas：拉格朗日乘子\n",
    "\n",
    "data：特征数据集\n",
    "\n",
    "label：对应的标签数据\n",
    "\n",
    "返回：\n",
    "\n",
    "W：权重向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k35sIzVwpDyM"
   },
   "outputs": [],
   "source": [
    "def ComputeW(alphas,data,label):\n",
    "    Data = mat(data)                 #转换为矩阵形式\n",
    "    Label = mat(label).transpose()\n",
    "    \n",
    "    m,n = shape(Data)                #数据的维度\n",
    "    w = zeros((n,1))\n",
    "    ### START THE CODE ###\n",
    "    \n",
    "    \n",
    "    ### END THE CODE ###\n",
    "    return w\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WXDChas5pDyP"
   },
   "outputs": [],
   "source": [
    "Testalphas =[[0.        ],[0.        ],[0.08999025],[0.        ],[0.04439791]]\n",
    "w = ComputeW(Testalphas,TrainData[50:55],TrainLabel[50:55])\n",
    "print (\"w = \", w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DTy5XOTOpDyS"
   },
   "source": [
    "输出：  \n",
    "\n",
    "w =  [[-0.02568303]<br>\n",
    " [ 0.04319313]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H9CS12lBpDyS"
   },
   "source": [
    "**任务七 画出SVM的决策边界**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PC_TGJ3SpDyT"
   },
   "source": [
    "定义PlotSVM函数，根据训练数据，标签，W，b,alphas画出决策边界。  \n",
    "正负样本用不同颜色标注。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sO8PFhwXpDyT"
   },
   "outputs": [],
   "source": [
    "W = ComputeW(alphas,TrainData,TrainLabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FtkQ86ftpDyW"
   },
   "outputs": [],
   "source": [
    "def PlotSVM(data,label,W,b,alphas):\n",
    "    Data = mat(data)\n",
    "    Label = np.squeeze(label)\n",
    "    #b 原来是矩阵 先转化为数组类型后其数组大小为（1，1），然后后面加[0],变为（1，）\n",
    "    b = array(b)[0]\n",
    "    fig = plt.figure()\n",
    "    figure = fig.add_subplot(111)\n",
    "    \n",
    "    figure.scatter(Data[:,0].flatten().A[0],Data[:,1].flatten().A[0])\n",
    "    x = arange(-1.0,10.0,0.1)\n",
    "    \n",
    "    y = (-b-W[0,0]*x)/ W[1,0]\n",
    "    figure.plot(x,y)\n",
    "    \n",
    "    ### START THE CODE ### \n",
    "\n",
    "\n",
    "    ###END THE CODE ###\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gT2UL0wjpDya"
   },
   "outputs": [],
   "source": [
    "PlotSVM(TrainData,TrainLabel,W,b,alphas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TgiEf6UNpDyf"
   },
   "source": [
    "**径向基函数是SVM中常用的一个核函数。径向基函数是一个采用向量作为自变量的函数，能够基于向量距离输出一个标量。这个距离可以是从<0,0>向量或者其他向量开始计算的距离。接下来，我们将会使用到径向基函数的高斯版本，其具体公式如下：**\n",
    "$$k(x_1,x_2)=exp(\\frac{-{{\\Vert}x_1-x_2\\Vert}^2}{2\\delta^2})$$\n",
    "其中,$\\delta$是用户定义的用于确定达到率或则说函数值跌落到0的速度参数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MYYlSJYxpDyg"
   },
   "source": [
    "**任务八 实现径向基核函数**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IcizRL1OpDyh"
   },
   "source": [
    "函数名：KernelTransform(Data,DataI,Para)  \n",
    "作用：将数据映射到高纬空间  \n",
    "参数：  \n",
    "Data 数据集  \n",
    "DataI 数据集中的第i行数据  \n",
    "papa：径向基函数中的$\\delta$参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DuAI8e9lpDyi"
   },
   "outputs": [],
   "source": [
    "def KernelTransform(Data,DataI,Para):\n",
    "    #计算Data的维度 【m,n】\n",
    "    m,n = shape(Data)\n",
    "    K = mat(zeros((m,1)))\n",
    "    \n",
    "    ### START THE CODE ###\n",
    "    \n",
    "\n",
    "    ### END THE CODE ###\n",
    "    return K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ylI1QBBspDym"
   },
   "outputs": [],
   "source": [
    "TestData =  [[3.542485, 1.977398], [3.018896, 2.556416], [7.55151, -1.58003], [2.114999, -0.004466], [8.127113, 1.274372]]\n",
    "TestDataI = [3.018896, 2.556416]\n",
    "TestPara = 0.8\n",
    "Result = KernelTransform(TestData,TestDataI,TestPara)\n",
    "print (\"Result = \", Result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nz7igXXdpDyp"
   },
   "source": [
    "输出：  \n",
    "Result =  [[6.21201706e-01]<br>\n",
    " [1.00000000e+00]<br>\n",
    " [1.67499988e-13]<br>\n",
    " [3.14534050e-03]<br>\n",
    " [3.88031058e-10]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OkZUvprdpDyp"
   },
   "source": [
    "**接下来我们导入KernnelTrainData数据进进行训练，每一行包括两个特征以及一个标签。然后利用SMO算法计算出拉格朗日乘子以及b，利用核函数转换计算K。最后进行预测并计算出预测错误率。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZxEFhUgvpDyq"
   },
   "outputs": [],
   "source": [
    "def TrainAccuracy(TrainFileName,TestFileName,P,C,Toler,MaxIter):\n",
    "    #导入数据\n",
    "    Data,Label = LoadData(TrainFileName)\n",
    "\n",
    "    #高斯核参数\n",
    "    Para = P\n",
    "    #转换为mat格式\n",
    "    Data = mat(Data)\n",
    "    Label = mat(Label).transpose()\n",
    "\n",
    "    #计算拉格朗日乘子以及b\n",
    "    oS = DataOp(Data,Label,C,Toler)\n",
    "    b,alphas = Smo(oS,MaxIter)\n",
    "\n",
    "    #获取alpha>0的行数\n",
    "    UnZero = nonzero(alphas.A > 0)[0]\n",
    "    SelectData = Data[UnZero]\n",
    "    SelectLabel = Label[UnZero]\n",
    "    SelectAlphas = alphas[UnZero]\n",
    "\n",
    "    #获取Data的维度\n",
    "    m, n = shape(Data)\n",
    "    \n",
    "    #获取测试数据集\n",
    "    TestData,TestLabel = LoadData(TestFileName)\n",
    "    TestCount = 0\n",
    "    \n",
    "    #转换格式\n",
    "    TestData = mat(TestData)\n",
    "    TestLabel = mat(TestLabel).transpose()\n",
    "    m,n = shape(TestData)\n",
    "    #遍历测试集每一行数据\n",
    "    for i in range(m):\n",
    "        #核函数转换\n",
    "        K = KernelTransform(SelectData,TestData[i],Para)\n",
    "        TestPredictValue = K.T*multiply(SelectLabel,SelectAlphas) + b\n",
    "        #测试准确度\n",
    "        if sign(TestPredictValue) != sign(TestLabel[i]):\n",
    "            TestCount += 1\n",
    "    print(\"The Test Error Rate is: %.1f%%\" % (float(TestCount)*100 / m))\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yUc58QMIpDyv"
   },
   "outputs": [],
   "source": [
    "#训练数据\n",
    "TrainFileName = 'KernelTrainData.txt'\n",
    "#测试数据\n",
    "TestFileName = 'KernelTestData.txt'\n",
    "#SMO算法参数\n",
    "C = 210\n",
    "Toler = 0.0001\n",
    "MaxIter = 10000\n",
    "#径向基参数\n",
    "Para = 0.12\n",
    "TrainAccuracy(TrainFileName,TestFileName,Para,C,Toler,MaxIter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2E54kKbWpDy1"
   },
   "source": [
    "输出：<br>\n",
    "    The Test Error Rate is: 20.0%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SuDVk1OopDy3"
   },
   "source": [
    "**由以上结果可以看到，应用高斯函数进行SVM分类，我们达到了80左右%的准确率，事实上你可以修改参数，来获得更加优化的模型。**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zzYFuvK4pDy3"
   },
   "source": [
    "通过以上的学习，我想你已经对SVM的原理以及和核函数有了一定的了解。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SVR算法**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "传统的回归模型通常直接基于模型输出f(x)与真实输出y之间的差来计算损失，只有当两者完全相同时，损失才为0。而SVR加入了一个ε参数，意为我们可以容忍f(x)与y最多有ε偏差，如下图\n",
    "![9.png](image/9.PNG)\n",
    "也就是说，在虚线之间的部分不进行损失计算，他们的损失为0，而只计算虚线以外的点的损失，因此他的对应损失函数为：\n",
    "![10.png](image/10.png)\n",
    "对于SVM，可以看作优化问题为：\n",
    "![11.png](image/11.png)\n",
    "引入松弛因子$ξ_i$、$ξ_i^*$，SVR的优化问题变为：\n",
    "![12.png](image/12.png)\n",
    "其中$h_{w,b}(x)=w^Tx+b$\n",
    "然后引入拉格朗日乘子,得到对应拉格朗日函数：\n",
    "$$L(w,b,α,α^*,ξ,ξ^*,r_i,r_i^*)=\\frac{1}{2}{\\parallel w\\parallel}^2+C\\sum_{i=0}^{m}(ξ_i+ξ_i^*)-\\sum_{i=1}^{m}r_iξ_i-\\sum_{i=1}^{m}r_i^*ξ_i^*+\\sum_{i=1}^{m}α_i(h_{w,b}(x^i)-y^i-ε-ξ_i)+$$$$\\sum_{i=1}^{m}α_i^*(y^i-h_{w,b}(x^i)-ε-ξ_i^*)$$\n",
    "令函数中对应偏导为0，得到：\n",
    "![13.png](image/13.png)\n",
    "代入到拉格朗日函数中得到之关于$α_i$,$α_i^*$的函数，最大化该函数即得到SVR的对偶问题：\n",
    "![14.png](image/14.png)\n",
    "可以看出其仍为QP问题，KKT条件为：\n",
    "![15.png](image/15.png)\n",
    "因此可以进行求解，同时对于非线性回归也可引入核函数实现。下面实现SVR。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVR的实现使用scikit-learn库调用实现，scikit-learn库里集成了大量机器学习的常用方法，其中提供了基于libsvm的SVR解决方案。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**任务九 SVR：线性回归实现**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#初始化一系列随机样本\n",
    "np.random.seed(42)\n",
    "m = 50\n",
    "X = 2 * np.random.rand(m, 1)\n",
    "y = (4 + 3 * X + np.random.randn(m, 1)).ravel()  # 将多维数组降为一维\n",
    "\n",
    "#调用sklearn库，实现LinearSVR\n",
    "from sklearn.svm import LinearSVR\n",
    "### START THE CODE ###\n",
    "                            #设定epsilon = 0.3,方便观察效果，支持向量均位于容忍区域外侧\n",
    "\n",
    "\n",
    "### END THE CODE  ###\n",
    "\n",
    "\n",
    "#找到支持向量\n",
    "def find_support(svr,X,y):\n",
    "    y_pred=svr.predict(X)#计算预测值\n",
    "    margin=(np.abs(y-y_pred) >= svr.epsilon)#判断是否为支持向量\n",
    "    return np.argwhere(margin)\n",
    "\n",
    "svr.support_=find_support(svr,X,y)#获取对应的支持向量对应下标，注意将变量名改为你自己的命名\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "绘制SVR结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_svr(svr,X,y,axes):\n",
    "    xls=np.linspace(axes[0],axes[1],100).reshape(100,1)\n",
    "    y_pred=svr.predict(xls)\n",
    "    plt.plot(xls, y_pred, \"-\")\n",
    "    plt.plot(xls, y_pred + svr.epsilon, \"--\")  \n",
    "    plt.plot(xls, y_pred - svr.epsilon, \"--\")\n",
    "    plt.plot(X, y, \"bo\")\n",
    "    plt.scatter(X[svr.support_], y[svr.support_], s=180, facecolors='#FFAAAA')\n",
    "    plt.axis(axes)\n",
    "plt.figure(figsize=(9, 4))  # width, height in inches\n",
    "plot_svr(svr, X, y, [0, 2, 3, 11]) #注意第一个参数改为自己的命名"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过观察图像可以看出，SVR中的支持向量均位于容忍区域外侧"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**任务十 SVR：非线性回归**<br>\n",
    "非线性回归中，参数C越大，对错样本的惩罚程度越大，正则化项的作用会越小，模型越趋向于过拟合。\n",
    "反之，C越小，正则化效果越强，模型会更简单。\n",
    "在实验中同学们可以通过调整C参数观察回归结果的不同。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#初始化随机样本\n",
    "np.random.seed(42)\n",
    "m = 100\n",
    "X = 2 * np.random.rand(m, 1) - 1\n",
    "y = (0.2 + 0.1 * X + 0.5 * X**2 + np.random.randn(m, 1)/10).ravel()\n",
    "\n",
    "#实现SVR\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "### START THE CODE ###\n",
    "\n",
    "\n",
    " \n",
    "### END THE CODE  ###\n",
    "\n",
    "plt.figure(figsize=(9, 4))\n",
    "plot_svr(svr_poly, X, y, [-1, 1, 0, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对应此数据集，通过改变C参数可以看出C越小，图像弯曲程度越小，而C越大图像越弯曲，对此数据拟合效果越好。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "SVM.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
